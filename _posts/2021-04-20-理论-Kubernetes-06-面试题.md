---
layout: post
title: 理论-Kubernetes-06-面试题
date: 2021-04-20
tags: 理论-Kubernetes
---

### 1. 为什么有了docker还要用kubernetes

> 为了提高业务并发和高可用，会使用多台服务器，因此会面向这些问题：多容器开主机提供服务、多容器分布节点部署、多容器怎么升级、怎么高效管理这些容器

### 2. kubernetes到底是什么东西

> K8S本质上就是一组服务器集群，k8s可以在集群的各个节点上运行特定的docker容器

### 3. pod的特点

> 一个pod可以理解为是一个应用的实例，比如nginx，用来提供服务的，pod中容器始终部署在一个node节点上，当然可以选择性的让他运行在某一个节点上，pod中容器共享网络、共享存储资源，kubernetes直接管理pod而不是管理容器

### 4. 常见的容器类型

> - Infrastructure
> - Container
> - 基础容器（pause）

### 5. 常见的pod控制器

> **ReplicaSet:** 代用户创建指定数量的pod副本数量，确保pod副本数量符合预期状态，并且支持滚动式自动扩容和缩容功能。
>
> ReplicaSet主要三个组件组成：
> 　　（1）用户期望的pod副本数量
> 　　（2）标签选择器，判断哪个pod归自己管理
> 　　（3）当现存的pod数量不足，会根据pod资源模板进行新建
> 帮助用户管理无状态的pod资源，精确反应用户定义的目标数量，但是RelicaSet不是直接使用的控制器，而是使用Deployment。
>
> **Deployment：**工作在ReplicaSet之上，用于管理无状态应用，目前来说最好的控制器。支持滚动更新和回滚功能，还提供声明式配置。
>
> **DaemonSet：**用于确保集群中的每一个节点只运行特定的pod副本，通常用于实现系统级后台任务。比如ELK服务
> 特性：服务是无状态的
> 服务必须是守护进程
>
> **Job：**只要完成就立即退出，不需要重启或重建。
>
> **Cronjob：**周期性任务控制，不需要持续后台运行，
>
> **StatefulSet：**管理有状态应用

### 6. ClusterIP 的 A 记录格式

> **<**servicename**>**.<namespacename>.svc.cluster.local
>
> 将一组Pod关联起来，提供一个统一的入口，即使Pod地址发生改变，这个统一的入口也不会发生变化，从而保证用户访问不受影响

### 7. docker网络模型

> bridge：当Docker进程启动时，**会在主机上创建一个名为docker0的虚拟网桥**，此主机上启动的Docker容器会连接到这个虚拟网桥上。虚拟网桥的工作方式和物理交换机类似，这样主机上的所有容器就通过交换机连在了一个二层网络中。
>
> host：如果启动容器的时候使用host模式，那么这个容器将不会**获得一个独立的Network Namespace**，而是和宿主机共用一个Network Namespace。**容器将不会虚拟出自己的网卡，配置自己的IP等**，而是使用宿主机的IP和端口。**但是，容器的其他方面，如文件系统、进程列表等还是和宿主机隔离的**。
>
> container：这个模式指定新创建的容器和**已经存在的一个容器共享一个 Network Namespace**，而不是和宿主机共享。**新创建的容器不会创建自己的网卡，配置自己的 IP，而是和一个指定的容器共享 IP、端口范围等**。同样，两个容器除了网络方面，**其他的如文件系统、进程列表等还是隔离的。两个容器的进程可以通过 lo 网卡设备通信。**
>
> none：使用none模式，**Docker容器拥有自己的Network Namespace，但是，并不为Docker容器进行任何网络配置**。也就是说，这个Docker容器没有网卡、IP、路由等信息。**需要我们自己为Docker容器添加网卡、配置IP等**。

### 8. kubernetes通信方式

> 容器间的通信、pod之间通信、pod和service之间的通信、集权外部客户端访问集群内不service ip 通信

### 9. 常见的插件

> - CNI网络插件→Flannel/calico
> - 服务发现插件→Coredns
> - 服务暴露插件→Traefik
> - GUI管理插件→Dashhoard
>
> flannel的后端网络模型：vxlan、host-gw、UDP
>
> vxlan：使用内核中的vxlan模块进行封装报文，将虚拟网络的数据帧添加到vxlan首部，封装在物理网络的UDP报文中，以传统网络的通信方式传送UDP报文
>
> ，到达目的主机后去掉物理网络报文的头部信息以及vxlan首部，并交付给目的终端。
>
> host-gw：即Host Gateway，通过再借点上创建目标容器地址的路由直接完成报文转发，要求个节点必须在同一个2层网络，对报文转发性能要求较高的场景使用。说白了就是，各个node节点互相指向了一条2层网络的静态路由
>
> UDP：使用普通的UDP报文封装完成隧道转发

calico和flannel区别

一句话概括flannel和calico区别，flannel实现网络传输比较复杂，期间有很多次封包解包过程，calico实现网络传输模式比较简单，网络传输没有任何封包解包过程

flannel 需要在同一二层网络下使用，局限性比较高

calico可以在三层网络环境下使用，集群延伸性比较高

flannel 不能对pod进行网络隔离，因为处于同一个二层网络下，calico可以对pod进行网络隔离，因为他调用的是内核层面，使用IP tables可以进行acl访问控制

### 10. 常见的服务对外暴露

> - Ingress
>
> 客户端访问pod应用的实现方式：首先客户端访问ingress对外暴露的http或https，然后域名解析到定义的IP地址，之后再找到集群所用到的lvs或者iptable流量调度模型，然后再把流量转发到cluster IP，之后再找到pod IP。

### 11. pod的状态

> - Pedding：资源限制导致、污点导致、硬亲和性导致（挂起状态）
> - running：pod已经被调度到某节点上，并且所有容器都已经被kubelet创建完成。
> - succeeded：Pod中的所有容器都被成功终止，并且不会再重启
> - failed：Pod中的所有容器都已终止了，并且至少有一个容器是因为失败终止，也就是说，容器以非0状态推出或者被系统终止
> - Unknown：健康探测失败后或因为某些原因无法取得Pod的状态，通常是因为与Pod所在主机通信失败
> - imagepullbackoff：镜像拉取失败
> - createcontainerError：创建失败
> - containercreating：正在创建

### 12. pod的资源调度

> - 节点预选
> - 节点优选
> - 节点选定

### 13. 节点亲和性调度

> - 硬亲和性：必须满足条件，否则pod状态为pending
> - 软亲和性：尽量满足条件，当所有节点都无法满足是可以被调度

### 14. pod重启策略

> - Always：容器退出后后总是重启容器
> - OnFailure：容器异常退出状态码不是0时，重启容器
> - Never：容器退出永远不重启

### 15. 健康检测

> - livenessProbe： 探测让用户可以自定义判断容器是否健康的条件。如果探测失败，Kubernetes 就会重启容器
> - readinessProbe：Liveness 探测是重启容器；Readiness 探测则是将容器设置为不可用，不接收 Service 转发的请求。

### 16. secret的四种类型

> - Service Account
> - Opaque
> - dockerconfigjson
> - tls

### 17. 常见数据卷类型

> - EmptyDir：把宿主机的空目录挂载到pod, 会在宿主机上创建数据卷目录并挂在到容器中。这种方式，Pod被删除后，数据也会丢失
> - HostPath：把宿主机的真实存在的目录挂载到pod
> - NFS： 把nfs共享存储的目录挂载到pod
> - CephFS：流行的云环境存储解决方案，它开源、高可用、弹性伸缩。它具备awsElasticBlockStore陈述之所有特点，并且单个voluem可以被多个节点同时使用。用户首先搭建自己的cephfs环境，然后配置kubernetes集群与其对接，最后在pod中使用其提供的volume
>
> 特殊类型
>
> - configmap：ConfigMap与Secret类似，区别在于ConfigMap保存的是不需要加密的应用的配置信息等。和Secret用法几乎相同
> - secret：帮你把Pod想要访问的加密数据，存放到Etcd中，然后通过在Pod容器里挂载Volume的方式，访问这些Secret里保存的信息

### 18. kubernetes安全框架的三个阶段

> 鉴权、授权、准入控制

### 19. 普罗米修斯组件

> kube-state-metrics：用来收集k8s基本状态信息的监控代理（比如多少个master多少个node）
>
> node-exporter：用来收集k8s运算节点的基础信息的（比如内存cpu磁盘使用量、网络IO等）
>
> cadvisor：它是用来监控容器内部使用资源的重要工具，直接从容器外部进行探测容器到底消耗了多少资源
>
> blackbox-exporter：探明业务容器是否存活

### 20. 普罗米修斯基本原理

> Prometheus server通过HTTP协议周期性抓取被监控组件的状态，在监控主机的时候，由被监控端通过HTTP接口（exporter）发送信息，临时性的job通过中间网关（pushgetway）推送到Prometheus server,其余周期性长的job则使用静态方式直接推送metrics给Prometheus serve，其间可通过多图形界面支持展示，如触发告警，则通过告警管理器（Alertmanager）发送报警。

### 21. 普罗米修斯的特点

> - 多维度数据模型。
> - 灵活的查询语言。
> - 不依赖分布式存储，单个服务器节点是自主的。
> - 通过基于HTTP的pull方式采集时序数据。
> - 可以通过中间网关进行时序列数据推送。
> - 通过服务发现或者静态配置来发现目标服务对象。
> - 支持多种多样的图表和界面展示，比如Grafana等。

### 22. 普罗米修斯的告警状态

> - Inactive：这里什么都没有发生。
> - Pending：已触发阈值，但未满足告警持续时间（即rule中的for字段）
> - Firing：已触发阈值且满足告警持续时间。警报发送到Notification Pipeline，经过处理，发送给接受者这样目的是多次判断失败才发告警，减少邮件。

### 23. lvs四种工作模式简介

> - dr：直接路由模式
> - tun：隧道模式
> - nat：路由转发模式
> - fullnat模式

### 24.详细介绍：

#### （1）DR：直接路由模式

> - 请求由 LVS 接受，由真实提供服务的服务器（RealServer, RS）直接返回给用户，返回的时候不经过 LVS。
>   DR 模式下需要 LVS 和绑定同一个 VIP（RS 通过将 VIP 绑定在 loopback 实现）。
>
> - 一个请求过来时，LVS 只需要将网络帧的 MAC 地址修改为某一台 RS 的 MAC，该包就会被转发到相应的 RS 处理，注意此时的源 IP 和目标 IP 都没变，LVS 只是做了一下移花接木。
> - RS 收到 LVS 转发来的包，链路层发现 MAC 是自己的，到上面的网络层，发现 IP 也是自己的，于是这个包被合法地接受，RS 感知不到前面有 LVS 的存在。
> - 而当 RS 返回响应时，只要直接向源 IP（即用户的 IP）返回即可，不再经过 LVS。
> - DR 模式是性能最好的一种模式。

#### （2）TUN模式：隧道模式

> - 客户端将访问vip报文发送给LVS服务器；
> - LVS服务器将请求报文重新封装，发送给后端真实服务器；
> - 后端真实服务器将请求报文解封，在确认自身有vip之后进行请求处理；
> - 后端真实服务器在处理完数据请求后，直接响应客户端。

#### （3）NAT 模式：路由转发模式

> - NAT（Network Address Translation）是一种外网和内网地址映射的技术。
>
> - 多目标的DNAT(iptables)转换;它通过修改请求报文的目标IP地址（同时可能会修改目标端口)挑选出某Real Server的RIP地址实现转发； 在LVS负载均衡调度器上请求先发送给PREROUTING-->INPUT，然后经由监听在INPUT上的LVS程序强制将请求转发给 POSTROUTING
>
> - NAT 模式下，网络报的进出都要经过 LVS 的处理。LVS 需要作为 RS 的网关。
>   当包到达 LVS 时，LVS 做目标地址转换（DNAT），将目标 IP 改为 RS 的 IP。RS 接收到包以后，仿佛是客户端直接发给它的一样。
>
>   RS 处理完，返回响应时，源 IP 是 RS IP，目标 IP 是客户端的 IP。
>   这时 RS 的包通过网关（LVS）中转，LVS 会做源地址转换（SNAT），将包的源地址改为 VIP，这样，这个包对客户端看起来就仿佛是 LVS 直接返回给它的。客户端无法感知到后端 RS 的存在。

#### （4）FULLNAT模式：

> - 无论是 DR 还是 NAT 模式，不可避免的都有一个问题：LVS 和 RS 必须在同一个 VLAN 下，否则 LVS 无法作为 RS 的网关。
> - Full-NAT 相比 NAT 的主要改进是，在 SNAT/DNAT 的基础上，加上另一种转换，转换过程如下：

### 25. LVS相关术语

> - DS：Director Server。指的是前端负载均衡器节点
> - RS：Real Server。后端真实的工作服务器
> - VIP：向外部直接面向用户请求，作为用户请求的目标的IP地址
> - DIP：Director Server IP，主要用于和内部主机通讯的IP地址
> - RIP：Real Server IP，后端服务器的IP地址
> - CIP：Client IP，访问客户端的IP地址

### 26. LVS负载均衡十种算法

> (1). 轮循调度 rr
> 均等地对待每一台服务器，不管服务器上的实际连接数和系统负载
>
> (2). 加权轮调 wrr
> 调度器可以自动问询真实服务器的负载情况，并动态调整权值
>
> (3). 最少链接 lc
> 动态地将网络请求调度到已建立的连接数最少的服务器上
> 如果集群真实的服务器具有相近的系统性能，采用该算法可以较好的实现负载均衡
>
> (4). 加权最少链接 wlc
> 调度器可以自动问询真实服务器的负载情况，并动态调整权值
> 带权重的谁不干活就给谁分配，机器配置好的权重高
>
> (5). 基于局部性的最少连接调度算法 lblc
> 这个算法是请求数据包的目标 IP 地址的一种调度算法，该算法先根据请求的目标 IP 地址寻找最近的该目标 IP 地址所有使用的服务器，如果这台服务器依然可用，并且有能力处理该请求，调度器会尽量选择相同的服务器，否则会继续选择其它可行的服务器
>
> (6). 复杂的基于局部性最少的连接算法 lblcr
> 记录的不是要给目标 IP 与一台服务器之间的连接记录，它会维护一个目标 IP 到一组服务器之间的映射关系，防止单点服务器负载过高。
>
> (7). 目标地址散列调度算法 dh
> 该算法是根据目标 IP 地址通过散列函数将目标 IP 与服务器建立映射关系，出现服务器不可用或负载过高的情况下，发往该目标 IP 的请求会固定发给该服务器。
>
> (8). 源地址散列调度算法 sh
> 与目标地址散列调度算法类似，但它是根据源地址散列算法进行静态分配固定的服务器资源。
>
> (9). 最少期望延迟 sed
> 不考虑非活动链接，谁的权重大，优先选择权重大的服务器来接收请求，但权重大的机器会比较忙
>
> (10). 永不排队 nq
> 无需队列，如果有realserver的连接数为0就直接分配过去

### 27. Dockerfile
```
FROM             # 基础镜像，一切从这里构建
MAINTAINER       # 镜像是谁写的，姓名+邮箱
RUN              # 镜像构建的时候需要运行的命令
ADD              # 可以把本地宿主机的内容添加到容器里，类似-v映射卷
WORKDIR          # 运行目录
USER             # 运行用户
VOLUME           # 挂载的目录位置
EXPOSE           # 暴露端口位置
CMD              # 指定这个容器启动的时候要运行的命令，只有最后一个会生效，可被替代
ENTRYPOINT       # 指定这个容器启动的时候要运行的命令，可追加命令
ONBUILD          # 当构建一个被继承 DockerFile 这个时候就会运行 INBUILD 的指令，出发指令。
COPY             # 类似AD，将我们文件拷贝到镜像中
ENY              # 构建的时候设置环境变量！
```

### 28. pod 创建流程

一句话总结，授权认证、信息写入、队列等待、节点调度、容器创建

> 第一步：kubectl   create  po
>
> ​      首先进行认证后，kubectl会调用master  api创建对象的接口，然后向k8s  apiserver发出创建pod的命令
>
> 第二步：k8s  apiserver
>
> ​        apiserver收到请求后，并非直接创建pod，而是先创建一个包含pod创建信息的yaml文件，并将文件信息写入到etcd中（如果此处是用yaml文件创建pod，则这两步就可以忽略）
>
> 第三步：controller  manager
>
> ​        创建Pod的yaml信息会交给controller  manager ,controller  manager根据配置信息将要创建的资源对象（pod）放到等待队列中。
>
> 第四步：scheduler
>
> ​        scheduler 查看 k8s api ，类似于通知机制。首先判断：pod.spec.Node == null?
> 若为null，表示这个Pod请求是新来的，需要创建；然后进行预选调度和优选调度计算，找到最“闲”的且符合调度条件的node。最后将信息在etcd数据库中更新分配结果：pod.spec.Node = node2(设置一个具体的节点)同样上述操作的各种信息也要写到etcd数据库中。
>
>     分配过程需要两层调度：预选调度和优选调度
>
> （1）预选调度：一般根据资源对象的配置信息进行筛选。例如NodeSelector、HostSelector和节点亲和性等。
>
> （2）优选调度：根据资源对象需要的资源和node节点资源的使用情况，为每个节点打分，然后选出最优的节点创建资源对象（pod）。
>
> 第四步：kubelet
>
> 目标节点node2上的kubelet进程通过API Server，查看etcd数据库（kubelet通过API Server的WATCH接口监听Pod信息，如果监听到新的pod副本被调度绑定到本节点）监听到kube-scheduler产生的Pod绑定事件后获取对应的Pod清单，然后调用node1本机中的docker  api初始化volume、分配IP、下载image镜像，创建容器并启动服务
>
> 第五步：controller  manager
>
> controller  manager会通过API Server提供的接口实时监控资源对象的当前状态，当发生各种故障导致系统状态发生变化时，会尝试将其状态修复到“期望状态”

### 29. 简述k8s组件作用

> kubectl：客户端工具，用来提交创建资源
>
> kubelet：会在每一个节点上运行一个进程，端口是10250，主要工作是用来接收master的指令，管理pod中的容器，定期向api发送资源使用情况
>
> pod：最基本的管理单位，pod封装了一个或多个容器
>
> api-server：核心大脑，用来接受各种客户端的请求信息
>
> controller-manager：管理pod的启停创建副本更新等
>
> schedule：用来调度容器所落在哪个节点上的，其中分为节点预选、节点优选
>
> kube-proxy：网络代理或负载均衡器，主要作用就是负责service的实现
>
> etcd：元数据存储

### 30. kube-proxy 有什么模式，有什么区别

> kube-proxy工作模式分为两种：
>
> - userspace
>
> - iptables
>
> - ipvs
>
> userspace特点：会为每一个service创建一个监听端口，向cluster ip发送请求会被iptables重定向到kube-proxy监听端口上，然后kube-proxy将流量转发给pod并建立连接，由于每次进行转发处理时两次内核和用户空间之间的数据拷贝，效率较另外两种模式低一些
>
> iptables特点：同上面的该模式下会为每一个pod创建对应的iptables规则，直接将发向Cluster IP的请求重定向到一个Pod IP，服务之间通信通过iptables规则来实现service负载均衡，随着service数量增大，iptables模式由于线性查找匹配，全量更新等特点，其性能会下降
>
> ipvs特点：该模式和iptables类似，kube-proxy监控Pod的变化并创建相应的ipvs rules。ipvs也是在kernel模式下通过netfilter实现的，但采用了hash table来存储规则，因此在规则较多的情况下，Ipvs相对iptables转发效率更高。除此以外，ipvs支持更多的LB算法

### 31. dockerfile entrypoint cmd 和 k8s cmd args 关系

- dockefile

> entrypotint：容器启动后执行的命令,让容器执行表现的像一个可执行程序一样，docker run 时命令不会被覆盖，根参数 --entrypoint也可以覆盖
>
> cmd：是指定容器启动时要运行的命令，这和[docker](https://so.csdn.net/so/search?q=docker&spm=1001.2101.3001.7020) run命令启动容器时指定要运行的命令十分类似，docker run 时命令会覆盖dockefile中指定的命令

- k8s

> command、args两项实现覆盖Dockerfile中ENTRYPOINT的功能,具体的command命令代替ENTRYPOINT的命令行，args代表集体的参数。
>
> 以下是使用场景
>
> 1. 如果command和args均没有指定，那么则使用Dockerfile的配置。
>
> 2. 如果command没有指定，但指定了args，那么Dockerfile中配置的ENTRYPOINT的命令行会被执行，并且将args中填写的参数追加到ENTRYPOINT中。
>
> 3. 如果command指定了，但args没有写，那么Dockerfile默认的配置会被忽略，执行输入的command（不带任何参数，当然command中可自带参数）。
>
> 4. 如果command和args都指定了，那么Dockerfile的配置被忽略，执行command并追加上args参数。

```sh
[root@Master ~]# vim pod-busybox-command-args.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-busybox-command-args
  labels:
    name: pod-busybox-command-args
spec:
  containers:
  - name: busybox
    image: busybox
    imagePullPolicy: IfNotPresent
    command: ["/bin/sh","-c","sleep 3600"]

[root@Master ~]# kubectl create -f pod-busybox-command-args.yaml
pod/pod-busybox-command-args created
[root@Master ~]# kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
pod-busybox-command-args   1/1     Running   0          4s

删除pod
[root@Master ~]# kubectl delete pod pod-busybox-command-args
pod "pod-busybox-command-args" deleted

修改方式一：
[root@Master ~]# vim pod-busybox-command-args.yaml
    command: ["/bin/sh","-c","sleep 3600"]
    修改为
    command: ["/bin/sh"]
    args: ["-c","sleep 3600"]

[root@Master ~]# kubectl create -f pod-busybox-command-args.yaml
pod/pod-busybox-command-args created
[root@Master ~]# kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
pod-busybox-command-args   1/1     Running   0          4s
删除pod
[root@Master ~]# kubectl delete pod pod-busybox-command-args
pod "pod-busybox-command-args" deleted

修改方式二：
[root@Master ~]# vim pod-busybox-command-args.yaml
    command: ["/bin/sh","-c","sleep 3600"]
修改为
    args:
    - /bin/sh
    - -c
    - sleep 3600

[root@Master ~]# kubectl create -f pod-busybox-command-args.yaml
pod/pod-busybox-command-args created
[root@Master ~]# kubectl get pods
NAME                       READY   STATUS    RESTARTS   AGE
pod-busybox-command-args   1/1     Running   0          4s
删除pod
[root@Master ~]# kubectl delete pod pod-busybox-command-args
pod "pod-busybox-command-args" deleted
```
### 32. 容器夯不住

那可能就是服务运行时的pid 1 不是你要运行的服务

### 33. k8s endpoint

像一些服务是内网之间访问并不在k8s中，pod就没办法访问到，而endpoint就会起到一个搭桥作用，创建好endpoint之后在访问service就能访问到了

```sh
$ cat oracle-endpoint-service.yaml

apiVersion: v1
kind: Service
metadata:
  name: oracle-test
spec:
  ports:
    - port: 1521
      targetPort: 1521
      protocol: TCP
---
kind: Endpoints
apiVersion: v1
metadata:
  name: oracle-test
subsets:
  - addresses:
      - ip: 172.100.1.93
    ports:
      - port: 1521

$ telnet oracle-test.default.svc.cluster.local:1521
```

### 34. 机器A上面save了一个镜像，拷贝到机器B并load进去，发现这个镜像没有tag，是什么原因

docker save 的时候使用的是镜像的id号，并不是全名称

### 35. 弹性缩容

Horizontal Pod Autoscaling（Pod 水平自动伸缩），简称HPA，HPA 通过监控分析一些控制器控制的所有 Pod 的负载变化情况来确定是否需要调整 Pod 的副本数量

```sh
# 创建一个 deployment
$ vim hpa-demo.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hpa-demo
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        resources:
          requests:
            memory: 50Mi
            cpu: 50m
---
apiVersion: v1
kind: Service
metadata:
  name: hpa-demo
  labels:
    k8s-app: nginx
spec:
  type: NodePort
  selector:
    k8s-app: nginx
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30080
```
创建服务和hpa
```sh
$ kubectl apply -f hpa-demo.yaml

$ kubectl autoscale deployment hpa-demo --cpu-percent=10 --min=1 --max=10

$ kubectl get hpa hpa-demo
NAME     REFERENCE           TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
hpa-demo   Deployment/hpa-demo   2%/10%    1         10        1          6m46s
```

增大负载进行测试

```sh
$ kubectl get svc
NAME     TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
hpa-demo   NodePort   10.110.250.132   <none>        80:31180/TCP   5d2h
$ while true; do wget -q -O- http://10.110.250.132; done
```

可以看到，HPA 已经开始工作

```sh
因为有时间性探测，所以一开始不会这么搞，要等待一会
$ kubectl get hpa
NAME       REFERENCE             TARGETS    MINPODS   MAXPODS   REPLICAS   AGE
hpa-demo   Deployment/hpa-demo   452%/10%   1         10        1          2m7s

$ kubectl get pods
NAME                        READY   STATUS              RESTARTS   AGE
hpa-demo-69968bb59f-8hjnn   1/1     Running             0          22s
hpa-demo-69968bb59f-9ss9f   1/1     Running             0          22s
hpa-demo-69968bb59f-bllsd   1/1     Running             0          22s
hpa-demo-69968bb59f-lnh8k   1/1     Running             0          37s
hpa-demo-69968bb59f-r8zfh   1/1     Running             0          22s
hpa-demo-69968bb59f-twtdp   1/1     Running             0          6m43s
hpa-demo-69968bb59f-w792g   1/1     Running             0          37s
hpa-demo-69968bb59f-zlxkp   1/1     Running             0          37s
hpa-demo-69968bb59f-znp6q   0/1     ContainerCreating   0          6s
hpa-demo-69968bb59f-ztnvx   1/1     Running             0          6s
```

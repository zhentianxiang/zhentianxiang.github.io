---
layout: post
title: 2023-02-14-k8s-1.25.1版本-CKA 考试题库
date: 2023-02-14
tags: 其他
music-id: 1330348068
---

# CKA 考试题库

模拟环境是在 node01（ 11.0.1.112 ）上做题的。

帐号为 candidate

密码为 123

在此账号下，可以免密 ssh 登录 master01 和 node02（ ssh master01 或 ssh node02 ）

3 台虚拟机的 candidate 帐号也可以免密切换到 root（ sudo -i ）

考试时，是 candidate 或 student 用户，在 node-1 机器上做题的，也不是在 root 的，也不是在 master 的，要注意。

## 1. 权限控制 RBAC

### 1.1 考题

> 设置配置环境：
>
> [candidate@node-1] $ kubectl config use-context k8s
> Context
>
> 为部署流水线创建一个新的 ClusterRole 并将其绑定到范围为特定的 namespace 的特定 ServiceAccount。
>
> **Task**
>
> 创建一个名为 deployment-clusterrole 且仅允许创建以下资源类型的新 ClusterRole：
>
> Deployment
> StatefulSet
> DaemonSet
>
> 在现有的 namespace app-team1 中创建一个名为 cicd-token 的新 ServiceAccount。
>
> 限于 namespace app-team1 中，将新的 ClusterRole deployment-clusterrole 绑定到新的 ServiceAccount cicd-token。

### 1.2 解答

考试时务必执行，切换集群。模拟环境中不需要执行
因为模拟环境只有一套集群，所以不存在名为 k8s 的 context

```sh
candidate@node01:~$ kubectl config use-context k8s
error: no context exists with the name: "k8s"
```

开始操作

1. 首先创建了集群角色，并且权限为创建 deployment、statefulset、daemonset

```sh
candidate@node01:~$ kubectl create clusterrole deployment-clusterrole --verb=create --resource=deployments,statefulset,daemonset
clusterrole.rbac.authorization.k8s.io/deployment-clusterrole created
```

2. 在 app-team1 命名空间下创建了一个服务账号

```sh
candidate@node01:~$ kubectl create serviceaccount -n app-team1 cicd-token
serviceaccount/cicd-token created
```

3. 在 app-team1 命名空间下创建一个角色绑定，并且将集群角色绑定到服务账号下

```sh
candidate@node01:~$ kubectl create rolebinding cicd-token-rolebinding -n app-team1 --clusterrole=deployment-clusterrole --serviceaccount=app-team1:cicd-token
rolebinding.rbac.authorization.k8s.io/cicd-token-rolebinding created
```

4. 检查

```sh
candidate@node01:~$ kubectl describe rolebinding -n app-team1 cicd-token-rolebinding
candidate@node01:~$ kubectl describe clusterrole deployment-clusterrole
```

## 2. 查看 pod 的 CPU

### 2.1 考题

> **设置配置环境：**
>
> [candidate@node-1] $ kubectl config use-context k8s
>
> **Task**
>
> 通过 pod label name=cpu-loader，找到运行时占用大量 CPU 的 pod，
>
> 并将占用 CPU 最高的 pod 名称写入文件 /opt/KUTR000401/KUTR00401.txt（已存在）。
>
> 考点：kubectl top --l 命令的使用
>
> 帮助：kubectl top pods -h

### 2.2 解答

```shell
# 考试时务必执行，模拟环境不需要
candidate@node01:~$ kubectl config use-context k8s

# 查看标签名为 name=cpu-loader 的 pod
# --sort-by=cpu 已 CPU 高低进行排序
# -A 全部的 pod
candidate@node01:~$ kubectl top pods -l name=cpu-loader --sort-by=cpu -A
NAMESPACE   NAME                          CPU(cores)   MEMORY(bytes)   
cpu-top     redis-test-5b65449f5b-4q7nz   1m           6Mi             
cpu-top     nginx-host-57b7655489-j7jv4   0m           2Mi             
cpu-top     test0-7f67c9777-rhqqv         0m           5Mi
candidate@node01:~$ echo "redis-test-5b65449f5b-4q7nz" >/opt/KUTR000401/KUTR00401.txt
candidate@node01:~$ cat /opt/KUTR000401/KUTR00401.txt
redis-test-5b65449f5b-4q7nz
```

## 3. 配置网络策略 NetworkPolicy

### 3.1 考题

> **设置配置环境：**
>
> [candidate@node-1] $ kubectl config use-context hk8s
>
> **Task**
>
> 在现有的 namespace my-app 中创建一个名为 allow-port-from-namespace 的新 NetworkPolicy。
>
> 确保新的 NetworkPolicy 允许 namespace echo 中的 Pods 连接到 namespace my-app 中的 Pods 的 9000 端口。
>
> 进一步确保新的 NetworkPolicy：
>
> **不允许**对没有在监听端口 9000 的 Pods 的访问
>
> **不允许**非来自 namespace echo 中的 Pods 的访问
>
> 双重否定就是肯定，所以最后两句话的意思就是：
>
> 仅允许端口为 9000 的 pod 访问。
>
> 仅允许 echo 命名空间中的 pod 访问。

考点：NetworkPolicy 的创建

**参考链接**

依次点击 Concepts → Services, Load Balancing, and Networking → Network Policies（看不懂英文的，可右上角翻译成中文）

https://kubernetes.io/zh-cn/docs/concepts/services-networking/network-policies/

![](/images/posts/other/CKA考试题库/1.png)

该网络策略示例：

1. default 命名空间下标签 role=db 的 所有 pod（如果它们不是已经被隔离的话）。
2. （Ingress 规则）允许以下 Pod 连接到 `default` 名字空间下的带有 `role=db` 标签的所有 Pod 的 6379 TCP 端口：
   - `default` 名字空间下带有 `role=frontend` 标签的所有 Pod 和 带有 `project=myproject` 标签的所有命名空间中的 Pod
   - IP 地址范围为 172.17.0.0–172.17.0.255 和 172.17.2.0–172.17.255.255 （即，除了 172.17.1.0/24 之外的所有 172.17.0.0/16)
3. （Egress 规则）允许 `default` 名字空间中任何带有标签 `role=db` 的 Pod 到 CIDR 10.0.0.0/24 下 5978 TCP 端口的连接。

### 3.2 解答

> 考试时务必执行，切换集群。模拟环境中不需要执行。
>
> \# kubectl config use-context hk8s
>
> 参考官方文档，拷贝 yaml 文件内容，并修改。
>
> 这里要注意，模拟题和真题截图里都有提到 echo 这个 namespace，但是和真题的截图比较，你会发现，两个 echo 出现的位置不同，一个作为访问者，一个作为
>
> 被访问者。
>
> 所以不要搞混了。他们其实只是个名字而已，叫什么都无所谓，但要分清访问和被访问。
>
> 开始操作
>
> 查看所有 ns 的标签 label
>
> kubectl get ns --show-labels
>
> 如果访问者的 namespace 没有标签 label，则需要手动打一个。如果有一个独特的标签 label，则也可以直接使用。
>
> kubectl label ns echo project=echo

检查 echo 命名空间是有的

```sh

candidate@node01:~$ kubectl get ns echo --show-labels
NAME   STATUS   AGE    LABELS
echo   Active   109d   kubernetes.io/metadata.name=echo
```

编写一个 yaml 文件用来创建

```yaml
candidate@node01:~$ vim networkpolicy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-port-from-namespace  # 要求的是这个名字
  namespace: my-app     # 要求在这个命名空间下
spec:
  podSelector:
    matchLabels: {}    # 要求 my-app 下的所有 pod,所以这里要写空,表示所有 pod
  policyTypes:
    - Ingress     # 要求入栈流量
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              project: echo    # 要访问者的命名空间的标签 label
        #- podSelector: {}     # 注意, 这个不要写,因为人家要求的是不允许非 namespace echo 中的 Pods 的访问,也就是只允许 namespace echo 中的 Pods 的访问
      ports:
        - protocol: TCP
          port: 9000   # 要访问的端口
```

提交资源

```shell
candidate@node01:~$ kubectl apply -f networkpolicy.yaml
networkpolicy.networking.k8s.io/allow-port-from-namespace created
```

检查

```shell
candidate@node01:~$ kubectl describe networkpolicies -n my-app allow-port-from-namespace
Name:         allow-port-from-namespace
Namespace:    my-app
Created on:   2023-02-14 10:39:00 +0800 CST
Labels:       <none>
Annotations:  <none>
Spec:
  PodSelector:     <none> (Allowing the specific traffic to all pods in this namespace)
  Allowing ingress traffic:
    To Port: 9000/TCP
    From:
      NamespaceSelector: project=echo
  Not affecting egress traffic
  Policy Types: Ingress
```

## 4. 暴露服务 service

### 4.1 考题

> **设置配置环境：**
>
> [candidate@node-1] $ kubectl config use-context k8s
>
> **Task**
>
> 请重新配置现有的 deployment front-end 以及添加名为 http 的端口规范来公开现有容器 nginx 的端口 80/tcp。
>
> 创建一个名为 front-end-svc 的新 service，以公开容器端口 http。
>
> 配置此 service，以通过各个 Pod 所在的节点上的 NodePort 来公开他们。

考点：将现有的 deploy 暴露成 nodeport 的 service

**参考连接**

依次点击 Concepts → Workloads → Workload Resources → Deployments（看不懂英文的，可右上角翻译成中文）

https://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/deployment/

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
          protocol: TCP # 不写也没事，默认就是 TCP
          name: http # 但是一定要写 name
```

### 4.2 解答

首先 edit 编辑现有的 front-end 添加名为 http 的端口 name

```yaml
candidate@node01:~$ kubectl edit deployments.apps front-end
······
        name: nginx  # 第39行下面添加4行内容
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
······
```

![](/images/posts/other/CKA考试题库/2.png)

暴露对应的端口，可以使用yaml文件来创建，也可以使用命令来创建

```yaml
candidate@node01:~$ vim front-end-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: front-end
spec:
  type: NodePort
  selector:
    app: front-end
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
```

```sh

candidate@node01:~$ kubectl apply -f front-end-svc.yaml
service/front-end created
candidate@node01:~$ kubectl get svc front-end-svc -o wide
NAME            TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE   SELECTOR
front-end-svc   NodePort   10.98.207.186   <none>        80:31989/TCP   38s   app=front-end
```

命令创建

```shell
candidate@node01:~$ kubectl expose deployment front-end --type=NodePort --port=80 --target-port=80 --name=front-end-svc
service/front-end-svc exposed
candidate@node01:~$ kubectl get svc front-end-svc -o wide
NAME            TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE   SELECTOR
front-end-svc   NodePort   10.98.207.186   <none>        80:31989/TCP   38s   app=front-end
```

验证

```
candidate@node01:~$ curl 10.98.207.186
Hello World ^_^
```

## 5. 创建 ingress

### 5.1 考题

> **设置配置环境：**
>
> [candidate@node-1] $ kubectl config use-context k8s
>
> **Task**
>
> 如下创建一个新的 nginx Ingress 资源：
>
> 名称: ping
>
> Namespace: ing-internal
>
> 使用服务端口 5678 在路径 /hello 上公开服务 hello
>
> 可以使用以下命令检查服务 hello 的可用性，该命令应返回 hello：
>
> curl -kL <INTERNAL_IP>/hello

考点：Ingress 的创建

依次点击 Concepts → Services, Load Balancing, and Networking → Ingress （看不懂英文的，可右上角翻译成中文）

https://kubernetes.io/zh-cn/docs/concepts/services-networking/ingress/

![](/images/posts/other/CKA考试题库/3.png)

![](/images/posts/other/CKA考试题库/4.png)

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: minimal-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx-example
  rules:
  - http:
      paths:
      - path: /testpath
        pathType: Prefix
        backend:
          service:
            name: test
            port:
              number: 80
```

### 5.2 解答

首先检查一下有没有默认的 clash

```sh
candidate@node01:~$ kubectl get ingressclasses.networking.k8s.io
NAME    CONTROLLER             PARAMETERS   AGE
nginx   k8s.io/ingress-nginx   <none>       104d
```

如果没有默认的 clash 则自己创建

```yaml
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  labels:
    app.kubernetes.io/component: controller
  name: nginx-example
  annotations:
    ingressclass.kubernetes.io/is-default-class: "true"
spec:
  controller: k8s.io/ingress-nginx
```

新建ingress yaml 文件

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ping
  namespace: ing-internal
  annotations:
#因为考试环境有多套,不清楚具体抽中的是哪套,在 1.25 的考试里,先写上这行,如果 apply 时报错需要指定域名,则注释这行再 apply,就成功了
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx  # 注意 clash 名称
  rules:
  - http:
      paths:
      - path: /hello      # 要求访问 hello 路径
        pathType: Prefix
        backend:
          service:
            name: hello
            port:
              number: 5678
```

提交

```shell
candidate@node01:~$ kubectl apply -f  ingress.yaml
ingress.networking.k8s.io/ping created
```

检查

```shell
candidate@node01:~$ kubectl get ingress -n ing-internal
NAME   CLASS   HOSTS   ADDRESS       PORTS   AGE
ping   nginx   *       10.108.39.2   80      3m16s
candidate@node01:~$ curl 10.108.39.2/hello
Hello World ^_^
```

## 6. 扩容 deployment 副本数量

### 6.1 考题

扩容 presentation 副本数量

没必要参考网址，使用-h 帮助更方便。

kubectl scale deployment -h

https://kubernetes.io/zh-cn/docs/tasks/run-application/scale-stateful-set/

### 6.2 解答

```sh
candidate@node01:~$ kubectl scale deployment presentation --replicas=4
deployment.apps/presentation scaled
candidate@node01:~$ kubectl get deployments.apps presentation
NAME           READY   UP-TO-DATE   AVAILABLE   AGE
presentation   4/4     4            4           104d
```

## 7. 调度 pod 到指定节点

### 7.1 考题

> 设置配置环境：
> [candidate@node-1] $ kubectl config use-context k8s
> Task
> 按如下要求调度一个 pod：
> 名称：nginx-kusc00401
> Image：nginx
> Node selector：disk=ssd

考点：nodeSelect 属性的使用

**参考链接**

（需要复制网页内容）

依次点击 Tasks → Configure Pods and Containers → Assign Pods to Nodes （看不懂英文的，可右上角翻译成中文）

https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/assign-pods-nodes/

![](/images/posts/other/CKA考试题库/5.png)

### 7.2 解答

查看是否存有相关信息

```sh
candidate@node01:~$ kubectl get pods -A |grep nginx-kusc00401
candidate@node01:~$ kubectl get node --show-labels |grep "disk=ssd"
node01     Ready    <none>          110d   v1.25.1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,disk=ssd,kubernetes.io/arch=amd64,kubernetes.io/hostname=node01,kubernetes.io/os=linux
```

如果没 pod 则需要创建 pod ，如果标签没有则创建标签`kubectl lable nodes node01 disk=ssd`，至于标签打到哪个节点无所谓，人家没要求

创建 yaml 文件

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-kusc-00401
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  nodeSelector:
    disk: ssd
```

提交资源

```sh
candidate@node01:~$ kubectl apply -f pod-disk-ssd.yaml
pod/nginx-kusc-00401 created
```

检查

```sh
candidate@node01:~$ kubectl get pods -o wide |grep nginx-kusc
nginx-kusc-00401               1/1     Running   0               20s     10.244.196.184   node01   <none>           <none>
```

## 8. 查看可用节点数量

### 8.1 考题

> **设置配置环境：**
>
> [candidate@node-1] $ kubectl config use-context k8s
>
> **Task**
>
> 检查有多少 nodes 已准备就绪（不包括被打上 Taint：NoSchedule 的节点），
>
> 并将数量写入 /opt/KUSC00402/kusc00402.txt

考点：检查节点角色标签，状态属性，污点属性的使用

**参考链接**

没必要参考网址，使用-h 帮助更方便。

kubectl -h

https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/

### 8.2 解答

> -i 忽略大小写
>
> -v 取反
>
> -c 以行计数

```sh
candidate@node01:~$ kubectl describe node |grep -i taints |grep -i -v -c noschedule > /opt/KUSC00402/kusc00402.txt
2
candidate@node01:~$ cat /opt/KUSC00402/kusc00402.txt
2
```

## 9. 创建多容器的 pod

### 9.1 考题

> **设置配置环境：**
>
> [candidate@node-1] $ kubectl config use-context k8s
>
> **Task**
>
> 按如下要求调度一个 Pod：
>
> 名称：kucc8
>
> app containers: 2
>
> container 名称/images：
>
> - nginx
>
> - consul

考点：pod 概念

**参考链接**

（需要复制网页内容）

依次点击 Concepts → Workloads → Pods （看不懂英文的，可右上角翻译成中文）

https://kubernetes.io/zh-cn/docs/concepts/workloads/pods/

### 9.2 解答

编写 yaml 文件

```yaml
candidate@node01:~$ vim pod-kucc.yaml

apiVersion: v1
kind: Pod
metadata:
  name: kucc8
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  - name: consul
    image: consul
    imagePullPolicy: IfNotPresent
```

提交资源

```sh
candidate@node01:~$ kubectl apply -f pod-kucc.yaml
pod/kucc8 created
```

检查

```sh
candidate@node01:~$ kubectl get pods kucc8
NAME    READY   STATUS    RESTARTS   AGE
kucc8   2/2     Running   0          8s
```

## 10. 创建 pv

### 10.1 考题

> **设置配置环境：**
>
> [candidate@node-1] $ kubectl config use-context hk8s
>
> **Task**
>
> 创建名为 app-config 的 persistent volume，容量为 1Gi，访问模式为 ReadWriteMany。
>
> volume 类型为 hostPath，位于 /srv/app-config

考点：hostPath 类型的 pv

**参考链接**

（需要复制网页内容）

依次点击 Tasks → Configure Pods and Containers → Configure a Pod to Use a PersistentVolume for Storage （看不懂英文的，可右上角翻译成中文）

https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-persistent-volume-storage/

![](/images/posts/other/CKA考试题库/6.png)

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
```

### 10.2 解答

创建 yaml 文件

考试时的访问模式可能有 ReadWriteMany 和 ReadOnlyMany 和 ReadWriteOnce，根据题目要求写

```yaml
candidate@node01:~$ vim pv.yaml

apiVersion: v1
kind: PersistentVolume
metadata:
  name: app-config
  #labels:
    #type: local
spec:
  #storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteMany
  hostPath:
    path: /srv/app-config
```

提交资源

```sh
candidate@node01:~$ kubectl apply -f pv.yaml
persistentvolume/app-config created
```

检查

```sh
candidate@node01:~$ kubectl get pv |grep app-config
app-config   1Gi        RWX            Retain           Available                                      40s
```

## 11. 创建 pvc

### 11.1 考题

> **设置配置环境：**
>
> [candidate@node-1] $ kubectl config use-context ok8s
>
> **Task**
>
> 创建一个新的 PersistentVolumeClaim：
>
> 名称: pv-volume
>
> Class: csi-hostpath-sc
>
> 容量: 10Mi
>
> 创建一个新的 Pod，来将 PersistentVolumeClaim 作为 volume 进行挂载：
>
> 名称：web-server
>
> Image：nginx:1.16
>
> 挂载路径：/usr/share/nginx/html
>
> 配置新的 Pod，以对 volume 具有 ReadWriteOnce 权限。
>
> 最后，使用 kubectl edit 或 kubectl patch 将 PersistentVolumeClaim 的容量扩展为 70Mi，并记录此更改。

考点：pvc 的创建 class 属性的使用，--record 记录变更

**参考链接**

（需要复制网页内容）

依次点击 Tasks → Configure Pods and Containers → Configure a Pod to Use a PersistentVolume for Storage （看不懂英文的，可右上角翻译成中文）

https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-persistent-volume-storage/

![](/images/posts/other/CKA考试题库/7.png)

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: task-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: task-pv-pod
spec:
  volumes:
    - name: task-pv-storage
      persistentVolumeClaim:
        claimName: task-pv-claim
  containers:
    - name: task-pv-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: task-pv-storage
```

### 11.2 解答

编辑 yaml 文件

```yaml
candidate@node01:~$ vim pvc.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pv-volume
spec:
  storageClassName: csi-hostpath-sc
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Mi
```

提交资源

```sh
candidate@node01:~$ kubectl apply -f pvc.yaml
persistentvolumeclaim/pv-volume created
```

检查

```sh
candidate@node01:~$ kubectl get pvc |grep pv-volume
pv-volume   Bound    pv01     10Mi       RWO            csi-hostpath-sc   8s
```

编辑 yaml 文件

```yaml
candidate@node01:~$ vim pvc-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: web-server
spec:
  volumes:
    - name: task-pv-storage
      persistentVolumeClaim:
        claimName: pv-volume
  containers:
    - name: nginx
      image: nginx:1.16
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: task-pv-storage
```

提交资源

```sh
candidate@node01:~$ kubectl apply -f pvc-pod.yaml
pod/web-server created
```

检查

```sh
candidate@node01:~$ kubectl get pods |grep web-server
web-server                     1/1     Running   0               92s
```

修改 pvc 大小为 70Mi

模拟环境是 nfs 存储，操作时，会有报错忽略即可。考试时用的动态存储，不会报错的

```
candidate@node01:~$ kubectl edit pvc pv-volume --record
```

![](/images/posts/other/CKA考试题库/8.png)

## 12. 查看 pod 日志

### 12.1 考题

> **设置配置环境：**
>
> [candidate@node-1] $ kubectl config use-context k8s
>
> **Task**
>
> 监控 pod foo 的日志并：
>
> 提取与错误 RLIMIT_NOFILE 相对应的日志行
>
> 将这些日志行写入 /opt/KUTR00101/foo

考点：kubectl logs 命令

**参考链接**

没必要参考网址，使用-h 帮助更方便。

kubectl -h

https://kubernetes.io/docs/tasks/debug/debug-application/debug-running-pod/#examine-pod-logs

### 12.2 解答

```sh
candidate@node01:~$ kubectl logs foo |grep "RLIMIT_NOFILE" > /opt/KUTR00101/foo
candidate@node01:~$ cat /opt/KUTR00101/foo
2023/02/14 00:42:31 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576
```

## 13. 使用 sidecar 代理容器日志

## 13.1 考题

> **设置配置环境：**
>
> [candidate@node-1] $ kubectl config use-context k8s
>
> **Context**
>
> 将一个现有的 Pod 集成到 Kubernetes 的内置日志记录体系结构中（例如 kubectl logs）。
>
> 添加 streaming sidecar 容器是实现此要求的一种好方法。
>
> **Task**
>
> 使用 busybox Image 来将名为 sidecar 的 sidecar 容器添加到现有的 Pod 11-factor-app 中。
>
> 新的 sidecar 容器必须运行以下命令：
>
> /bin/sh -c tail -n+1 -f /var/log/11-factor-app.log
>
> 使用挂载在/var/log 的 Volume，使日志文件 11-factor-app.log 可用于 sidecar 容器。
>
> 除了添加所需要的 volume mount 以外，请勿更改现有容器的规格。

考题翻译成白话，就是：

添加一个名为 sidecar 的边车容器(使用 busybox 镜像)，加到已有的 pod 11-factor-app 中。

确保 sidecar 容器能够输出 /var/log/11-factor-app.log 的信息。

使用 volume 挂载 /var/log 目录，确保 sidecar 能访问 11-factor-app.log 文件



考点：pod 两个容器共享存储卷

**参考链接**

（需要复制网页内容）

依次点击 Concepts → Cluster Administration → Logging Architecture （看不懂英文的，可右上角翻译成中文）

https://kubernetes.io/zh-cn/docs/concepts/cluster-administration/logging/

![](/images/posts/other/CKA考试题库/9.png)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: counter
spec:
  containers:
  - name: count
    image: busybox:1.28
    args:
    - /bin/sh
    - -c
    - >
      i=0;
      while true;
      do
        echo "$i: $(date)" >> /var/log/1.log;
        echo "$(date) INFO $i" >> /var/log/2.log;
        i=$((i+1));
        sleep 1;
      done      
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  - name: count-log-1
    image: busybox:1.28
    args: [/bin/sh, -c, 'tail -n+1 -F /var/log/1.log']
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  - name: count-log-2
    image: busybox:1.28
    args: [/bin/sh, -c, 'tail -n+1 -F /var/log/2.log']
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  volumes:
  - name: varlog
    emptyDir: {}
```

### 13.2 解答

备份原始 pod 信息，删除旧的 pod 11-factor-app

```sh
# 非常抱歉，我的模拟环境没有这个 pod
candidate@node01:~$ kubectl get pods -A |grep  11-factor-app
```

那我们只能来观察体验一下官方提供的模板了

```
candidate@node01:~$ kubectl apply -f count-log-1.yaml
pod/counter created
```

查看日志

```
candidate@node01:~$ kubectl logs counter count-log-1
0: Tue Feb 14 07:10:45 UTC 2023
1: Tue Feb 14 07:10:46 UTC 2023
2: Tue Feb 14 07:10:47 UTC 2023
3: Tue Feb 14 07:10:48 UTC 2023
```

```
candidate@node01:~$ kubectl logs counter count-log-2
Tue Feb 14 07:10:45 UTC 2023 INFO 0
Tue Feb 14 07:10:46 UTC 2023 INFO 1
Tue Feb 14 07:10:47 UTC 2023 INFO 2
Tue Feb 14 07:10:48 UTC 2023 INFO 3
```

我们来解释一下什么意思，一开始只有 count 一个容器，然后执行 while 循环命令来 echo 时间日期信息到/var/log/1.log文件中

然后又定义了一个 count-log-1 容器，然后 tail 一直在查看 1.log 文件

然后 count-log-2 再看 2.log 文件，因为他们都使用的同一个 volumeMounts，所以数据/var/log目录对于他们三个容器是共享的

## 14. 升级集群

### 14.1 考题

> **设置配置环境：**
>
> [candidate@node-1] $ kubectl config use-context mk8s
>
> **Task**
>
> 现有的 Kubernetes 集群正在运行版本 1.25.1。**仅将 master 节点**上的所有 Kubernetes 控制平面和节点组件升级到版本 1.25.2。
>
> 确保在升级之前 drain master 节点，并在升级后 uncordon master 节点。
>
> 可以使用以下命令，通过 ssh 连接到 master 节点：
>
> ssh master01
>
> 可以使用以下命令，在该 master 节点上获取更高权限：
>
> sudo -i
>
> 另外，在主节点上升级 kubelet 和 kubectl。
>
> 请不要升级工作节点，etcd，container 管理器，CNI 插件， DNS 服务或任何其他插件。

（注意，考试敲命令时，注意要升级的版本，根据题目要求输入具体的升级版本！！！）

考点：如何离线主机，并升级控制面板和升级节点

**参考链接**

没必要参考网址，建议多练习，背过命令就行。

记不清的，可以使用 kubectl -h 来帮助。

如果非要参考，可以按照下面方法。

依次点击 Tasks → Administer a Cluster → Administration with kubeadm → Upgrading kubeadm clusters （看不懂英文的，可右上角翻译成中文）

https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/

![](/images/posts/other/CKA考试题库/10.png)

### 12.2 解答

首先我们要先停止调度

```sh
candidate@node01:~$ kubectl cordon master01
node/master01 cordoned
candidate@node01:~$ kubectl get node
NAME       STATUS                     ROLES           AGE    VERSION
master01   Ready,SchedulingDisabled   control-plane   110d   v1.25.1
node01     Ready                      <none>          110d   v1.25.1
node02     Ready                      <none>          110d   v1.25.1
```

驱逐 pod

```sh
candidate@node01:~$ kubectl drain master01 --ignore-daemonsets
node/master01 already cordoned
Warning: ignoring DaemonSet-managed Pods: calico-system/calico-node-87nzw, kube-system/kube-proxy-25gqq
evicting pod kube-system/coredns-c676cc86f-dmbx6
evicting pod calico-system/calico-kube-controllers-65648cd788-w8phx
pod/calico-kube-controllers-65648cd788-w8phx evicted
pod/coredns-c676cc86f-dmbx6 evicted
node/master01 drained
```

ssh master 节点下，并切换 root

```sh
candidate@node01:~$ ssh master01
candidate@master01:~$ sudo -i
```

安装控制平面

```shell
root@master01:~# apt-get update
root@master01:~# apt-cache show kubeadm|grep 1.25.2
Version: 1.25.2-00
Filename: pool/kubeadm_1.25.2-00_amd64_5f3996559255107d2ad690fdded29e76bb6b550154622ccf22444753d2ab2272.deb
root@master01:~# apt-get install -y kubeadm=1.25.2-00
root@master01:~# kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"25", GitVersion:"v1.25.2", GitCommit:"5835544ca568b757a8ecae5c153f317e5736700e", GitTreeState:"clean", BuildDate:"2022-09-21T14:32:18Z", GoVersion:"go1.19.1", Compiler:"gc", Platform:"linux/amd64"}
```

验证升级计划，会显示可以升级很多版本，我们关注题目要求升级到哪个版本

```sh
root@master01:~# kubeadm upgrade plan
```

升级 kubeadm

排除 etcd，升级其他的，提示时输入 y

```sh
root@master01:~# kubeadm upgrade apply v1.25.2 --etcd-upgrade=false
```

![](/images/posts/other/CKA考试题库/11.png)

![12](/images/posts/other/CKA考试题库/12.png)

升级 kubelet

```sh
root@master01:~# apt-cache show kubelet |grep 1.25.2
Version: 1.25.2-00
Filename: pool/kubelet_1.25.2-00_amd64_19b51b4b7b3748163bb4305d1684bb69ccc5b5ad0a24694cf045e6a97ffbb815.deb
Size: 18251236
root@master01:~# apt-get -y install kubelet=1.25.2-00
root@master01:~# kubelet --version
Kubernetes v1.25.2
```

升级 kubectl

```sh
root@master01:~# apt-cache show kubectl |grep 1.25.2
Version: 1.25.2-00
Filename: pool/kubectl_1.25.2-00_amd64_f69e71defe39c333d60d5c01bdd26e44e8ac60df7ba4b3a31056bd9618d673bb.deb
root@master01:~# apt-get -y install kubectl=1.25.2-00
root@master01:~# kubectl version |grep 1.25.2
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"25", GitVersion:"v1.25.2", GitCommit:"5835544ca568b757a8ecae5c153f317e5736700e", GitTreeState:"clean", BuildDate:"2022-09-21T14:33:49Z", GoVersion:"go1.19.1", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"25", GitVersion:"v1.25.2", GitCommit:"5835544ca568b757a8ecae5c153f317e5736700e", GitTreeState:"clean", BuildDate:"2022-09-21T14:27:13Z", GoVersion:"go1.19.1", Compiler:"gc", Platform:"linux/amd64"}
```

退出 root------退出普通master01---------回到node01

```sh
# 按两下 CTRL + D
root@master01:~# logout
candidate@master01:~$ logout
Connection to master01 closed.
candidate@node01:~$
```

恢复 master01 调度

```sh
candidate@node01:~$ kubectl uncordon master01
node/master01 uncordoned
candidate@node01:~$ kubectl get node
NAME       STATUS   ROLES           AGE    VERSION
master01   Ready    control-plane   110d   v1.25.2
node01     Ready    <none>          110d   v1.25.1
node02     Ready    <none>          110d   v1.25.1
```

## 15. 备份还原 ETCD

### 15.1 考题

> **设置配置环境**
>
> 此项目无需更改配置环境。但是，在执行此项目之前，请确保您已返回初始节点。
>
> [candidate@master01] $ exit #注意，这个之前是在 master01 上，所以要 exit 退到 node01，如果已经是 node01 了，就不要再 exit 了。
>
> **Task**
>
> 首先，为运行在 https://11.0.1.111:2379 上的现有 etcd 实例创建快照并将快照保存到 /var/lib/backup/etcd-snapshot.db
>
> （注意，真实考试中，这里写的是 https://127.0.0.1:2379）
>
> 为给定实例创建快照预计能在几秒钟内完成。 如果该操作似乎挂起，则命令可能有问题。用 CTRL + C 来取消操作，然后重试。
>
> 然后还原位于/data/backup/etcd-snapshot-previous.db 的现有先前快照。
>
> 提供了以下 TLS 证书和密钥，以通过 etcdctl 连接到服务器。
>
> CA 证书: /opt/KUIN00601/ca.crt
>
> 客户端证书: /opt/KUIN00601/etcd-client.crt
>
> 客户端密钥: /opt/KUIN00601/etcd-client.key

考点：etcd 的备份和还原命令

**参考链接**

没必要参考网址，建议多练习，背过命令就行。

记不清的，可以使用 etcdctl -h 来帮助，更方便。

如果非要参考，可以按照下面方法。

依次点击 Tasks → Administer a Cluster → Operating etcd clusters for Kubernetes （看不懂英文的，可右上角翻译成中文）

https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/configure-upgrade-etcd/

![](/images/posts/other/CKA考试题库/13.png)

![](/images/posts/other/CKA考试题库/14.png)

### 15.2 解答

注意下面3种截然不同的做法，我强烈推荐使用第一种方法，稳妥些。对k8s比较熟悉的人也会简单排错的人可以选择第2种和第3种方法。

**方法1：**

> 首先可以确定，网上这种做法并不准确，但确实比较保险的做法。即使错了，也不会影响其他题目，且只会扣 4 分以内
>
> 另外，特别注意，etcd 这道题，在考试时做完后，就不要回头检查或者操作了。因为它是用的前一道题的集群，所以一旦你回头再做时，切换错集群了，且又将etcd 还原了，反而可能影响别的考题。
>
> 但如果事后回来做这道题的话，切记要切换为正确的集群。
>
> \# kubectl config use-context xxxx

**注意集群用的是考试时的上一题的集群，所以无需再切换集群了**

**开始操作**

#### 备份 ETCD 数据库

```sh
candidate@node01:~$ etcdctl --endpoints=https://11.0.1.111:2379 --cacert=/opt/KUIN00601/ca.crt --cert=/opt/KUIN00601/etcd-client.crt --key=/opt/KUIN00601/etcd-client.key snapshot save /var/lib/backup/etcd-snapshot.db
```

![](/images/posts/other/CKA考试题库/15.png)

检查一下为好

```sh
candidate@node01:~$ etcdctl snapshot status /var/lib/backup/etcd-snapshot.db -wtable
Deprecated: Use `etcdutl snapshot status` instead.

+----------+----------+------------+------------+
|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
+----------+----------+------------+------------+
| b4a5419f |   207550 |       1739 |      14 MB |
+----------+----------+------------+------------+
```

#### 还原数据库

> 考试时，/data/backup/etcd-snapshot-previous.db 的权限应该是只有 root 可读，所以需要使用 sudo 命令。
>
> 可以 ll /data/backup/etcd-snapshot-previous.db 检查一下读写权限和属主

```sh
candidate@node01:~$ sudo ETCDCTL_API=3 etcdctl --endpoints=https://11.0.1.111:2379 --cacert="/opt/KUIN00601/ca.crt" --cert="/opt/KUIN00601/etcd-client.crt" --key="/opt/KUIN00601/etcd-client.key" snapshot restore /data/backup/etcd-snapshot-previous.db
```

![](/images/posts/other/CKA考试题库/16.png)

**方法2：**

> （建议了解 K8S 原理的这么做，但前提是 etcd 使用的是 systemd 服务，而不是 etcd pod。）
>
> （真实考试时，使用的是 systemd 服务的。但模拟环境使用的是 etcd pod。无法在模拟环境模拟练习这道题的。）
>
> 方法 2 有一个前提，就是 etcd 使用的是 systemd 服务，而不是 etcd 容器。
>
> 1.22 和 1.23 的考试里，有人反馈 etcd 是服务，有人反馈 etcd 是 pod。我怀疑 CKA 可能有多套考试环境，所以你做的时候，需要确认是否为 etcd 服务。
>
> 这个模拟环境，使用的是 etcd pod 的方式搭建的，所以不能用此方法测试。
>
> 方法 2 和方法 3 类似，或者说原理是相同的。而且，是可以实现真正 etcd 快照还原的步骤：
>
> 首先说一下，这种做法稍后有风险的，一旦操作错误，集群出问题，很可能影响同集群的其他考题，所以不建议冒险
>
> 注意集群用的是考试时的上一题的集群，所以无需再切换集群了。
>
> 但如果事后回来做这道题的话，切记要切换为正确的集群。
>
> \# kubectl config use-context xxxx

备份集群

```sh
candidate@node01:~$ etcdctl --endpoints=https://11.0.1.111:2379 --cacert=/opt/KUIN00601/ca.crt --cert=/opt/KUIN00601/etcd-client.crt --key=/opt/KUIN00601/etcd-client.key snapshot save /var/lib/backup/etcd-snapshot.db
```

**还原：**

> 考试时，/data/backup/etcd-snapshot-previous.db 的权限应该是只有 root 可读，所以需要使用 sudo 命令。
>
> 可以 ll /data/backup/etcd-snapshot-previous.db 检查一下读写权限和属主。

1、先检查一下考试环境，使用的 etcd 是服务还是容器。

```sh
kubectl get pod -A
```

```sh
# 注意加 sudo，因为你用的 candidate 帐号，而非 root 帐号。
sudo systemctl status etcd
```

如果是 systemd 服务，则继续往下操作。（模拟环境使用的 etcd 不是服务，而是方法 3 的 pod。）

2、确认 etcd 数据目录（--data-dir 值）

```sh
ps -ef |grep etcd
```

一般默认为/var/lib/etcd

3、停止 etcd 服务

```sh
sudo systemctl stop etcd
```

4、先移动备份 etcd 原目录

```sh
sudo mv /var/lib/etcd /var/lib/etcd.bak
```

5、开始还原（还原时，可以不加证书和秘钥）

```sh
sudo ETCDCTL_API=3 etcdctl --data-dir=/var/lib/etcd snapshot restore /data/backup/etcd-snapshot-previous.db
```

6、更改文件属主

```sh
sudo chown -R etcd:etcd /var/lib/etcd
```

7、启动 etcd 服务

```sh
sudo systemctl start etcd
```

**方法3：**

> 方法 3 和方法 2 类似，或者说原理是相同的。而且，是可以实现真正 etcd 快照还原的步骤：
>
> 首先说一下，这种做法稍后有风险的，一旦操作错误，集群出问题，很可能影响同集群的其他考题，所以不建议冒险。
>
> 注意集群用的是考试时的上一题的集群，所以无需再切换集群了。
>
> 但如果事后回来做这道题的话，切记要切换为正确的集群。
>
> \# kubectl config use-context xxxx

备份集群

```sh
candidate@node01:~$ etcdctl --endpoints=https://11.0.1.111:2379 --cacert=/opt/KUIN00601/ca.crt --cert=/opt/KUIN00601/etcd-client.crt --key=/opt/KUIN00601/etcd-client.key snapshot save /var/lib/backup/etcd-snapshot.db
```

还原

> 考试时，/data/backup/etcd-snapshot-previous.db 的权限应该是只有 root 可读，所以需要使用 sudo 命令。
>
> 可以 ll /data/backup/etcd-snapshot-previous.db 检查一下读写权限和属主。

测试环境的 etcd 在 master01 节点，所以需要切换到 master 节点。

但考试时，从考试给出的题目“为运行在 https://11.0.1.111:2379 上的现有 etcd 实例创建快照”可知，etcd 是运行在本节点 node-1 上的。所以考试时，不需要切到 master。

1. 移除且备份目录

```sh
candidate@node01:~$ ssh master01
candidate@master01:~$ sudo mv /etc/kubernetes/manifests/ /etc/kubernetes/manifests.bak
candidate@master01:~$ crictl ps|grep etcd && crictl ps|grep kube-apiserver
WARN[0000] runtime connect using default endpoints: [unix:///var/run/dockershim.sock unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
ERRO[0000] unable to determine runtime API version: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /var/run/dockershim.sock: connect: no such file or directory"
ERRO[0000] unable to determine runtime API version: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/containerd/containerd.sock: connect: permission denied"
ERRO[0000] unable to determine runtime API version: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/crio/crio.sock: connect: no such file or directory"
ERRO[0000] unable to determine runtime API version: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /var/run/cri-dockerd.sock: connect: no such file or directory"
FATA[0000] unable to determine runtime API version: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /var/run/cri-dockerd.sock: connect: no such file or directory"
```

2. 备份现有的 etcd 数据

```sh
candidate@master01:~$ sudo mv /var/lib/etcd/ /var/lib/etcd.bak
```

3. 开始还原

```sh
candidate@master01:~$ sudo mkdir /data/backup
# 退出 master01机器
# 首先要把 node01 节点上备份的 etcd 数据文件 scp 到 master01 机器上，因为要在 master01 还原恢复操作
candidate@node01:~$ sudo scp /data/backup/etcd-snapshot-previous.db master01:/data/
etcd-snapshot-previous.db                                            100%   14MB 154.9MB/s   00:00
candidate@node01:~$ ssh master01
candidate@master01:~$ sudo ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert="/opt/KUIN00601/ca.crt" --cert="/opt/KUIN00601/etcd-client.crt" --key="/opt/KUIN00601/etcd-client.key" snapshot restore /var/lib/backup/etcd-snapshot-previous.db --data-dir=/var/lib/etcd/
# 恢复 Kube-Apiserver 与 Etcd 镜像
candidate@master01:~$ sudo mv /etc/kubernetes/manifests.bak/ /etc/kubernetes/manifests
```

4. 切换回 node01 进行查看

> 考试时不用，因为考试时都在 node01 上进行操作

```sh
candidate@master01:~$ logout
Connection to master01 closed.
candidate@node01:~$ kubectl get pods -A
NAMESPACE         NAME                                       READY   STATUS    RESTARTS       AGE
calico-system     calico-kube-controllers-65648cd788-75crz   1/1     Running   3 (109d ago)   110d
calico-system     calico-node-87nzw                          1/1     Running   3 (109d ago)   110d
calico-system     calico-node-bzv8r                          1/1     Running   3 (109d ago)   110d
calico-system     calico-node-pxcrw                          1/1     Running   3 (109d ago)   110d
calico-system     calico-typha-667cb46dc8-6hbxd              1/1     Running   3 (109d ago)   110d
calico-system     calico-typha-667cb46dc8-znxmg              1/1     Running   3 (109d ago)   110d
kube-system       coredns-c676cc86f-4tzwg                    1/1     Running   3 (109d ago)   110d
kube-system       coredns-c676cc86f-whv2h                    1/1     Running   4 (109d ago)   110d
kube-system       etcd-master01                              1/1     Running   0              110d
kube-system       kube-apiserver-master01                    1/1     Running   7 (109d ago)   110d
kube-system       kube-controller-manager-master01           1/1     Running   7 (109d ago)   110d
kube-system       kube-proxy-25gqq                           1/1     Running   7 (109d ago)   110d
kube-system       kube-proxy-ldrj7                           1/1     Running   7 (109d ago)   110d
kube-system       kube-proxy-x2gbq                           1/1     Running   7 (109d ago)   110d
kube-system       kube-scheduler-master01                    1/1     Running   7 (109d ago)   110d
tigera-operator   tigera-operator-7659dcc9f4-4976s           1/1     Running   19 (12m ago)   110d
```

不知道还原到哪去了，缺了很多 pod

![](/images/posts/other/CKA考试题库/17.png)

## 16. 排查集群中故障节点

### 16.1 考题

> **设置配置环境：**
>
> [candidate@node-1] $ kubectl config use-context wk8s
>
> **Task**
>
> 名为 node02 的 Kubernetes worker node 处于 NotReady 状态。
>
> 调查发生这种情况的原因，并采取相应的措施将 node 恢复为 Ready 状态，确保所做的任何更改永久生效。
>
> 可以使用以下命令，通过 ssh 连接到 node02 节点：
>
> ssh node02
>
> 可以使用以下命令，在该节点上获取更高权限：
>
> sudo -i



**参考链接**

没必要参考网址，记住先 restart 再 enable 就行。

https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/kubelet-integration/

### 16.2 解答

> 考试时务必执行，切换集群。模拟环境中不需要执行。
>
> \# kubectl config use-context wk8s
>
> 通过 get nodes 查看异常节点，登录节点查看 kubelet 等组件的 status 并判断原因。
>
> 真实考试时，这个异常节点的 kubelet 服务没有启动导致的，就这么简单。
>
> 执行初始化这道题的脚本 a.sh，模拟 node02 异常。
>
> （考试时，不需要执行的！考试时切到这道题的集群后，那个 node 就是异常的。）

首先模拟故障

```sh
candidate@node01:~$ sudo ./a.sh
Removed /etc/systemd/system/multi-user.target.wants/kubelet.service.
candidate@node01:~$ kubectl get node
NAME       STATUS     ROLES           AGE    VERSION
master01   Ready      control-plane   110d   v1.25.2
node01     Ready      <none>          110d   v1.25.1
node02     NotReady   <none>          110d   v1.25.1
```

解决问题

```sh
candidate@node01:~$ ssh node02
candidate@node02:~$ systemctl status kubelet
● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; disabled; vendor preset: enabled)
    Drop-In: /etc/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: inactive (dead) since Tue 2023-02-14 17:57:55 CST; 1min 53s ago
       Docs: https://kubernetes.io/docs/home/
   Main PID: 1424 (code=exited, status=0/SUCCESS)
candidate@node02:~$ sudo systemctl enable kubelet --now
Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service → /lib/systemd/system/kubelet.service.
```

检查

```sh
candidate@node02:~$ systemctl status kubelet
● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
    Drop-In: /etc/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: active (running) since Tue 2023-02-14 18:00:12 CST; 24s ago
       Docs: https://kubernetes.io/docs/home/
   Main PID: 130207 (kubelet)
      Tasks: 16 (limit: 2590)
     Memory: 47.0M
     CGroup: /system.slice/kubelet.service

# 回推到 node01 查看节点状态
candidate@node02:~$ logout
Connection to node02 closed.
candidate@node01:~$ kubectl get node
NAME       STATUS   ROLES           AGE    VERSION
master01   Ready    control-plane   110d   v1.25.2
node01     Ready    <none>          110d   v1.25.1
node02     Ready    <none>          110d   v1.25.1
```

## 17. 节点维护

### 17.1 考题

> **设置配置环境：**
>
> [candidate@node-1] $ kubectl config use-context ek8s
>
> **Task**
>
> 将名为 node02 的 node 设置为不可用，并重新调度该 node 上所有运行的 pods。

考点：cordon 和 drain 命令的使用

**参考链接**

没必要参考网址，使用-h 帮助更方便。

kubectl -h

https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/safely-drain-node/

### 17.2 解答

设置为不可调度

```
candidate@node01:~$ kubectl cordon node02
node/node02 cordoned
candidate@node01:~$ kubectl get node
NAME       STATUS                     ROLES           AGE    VERSION
master01   Ready                      control-plane   110d   v1.25.2
node01     Ready                      <none>          110d   v1.25.1
node02     Ready,SchedulingDisabled   <none>          110d   v1.25.1
```

疏散 pod

> 注意，还有一个参数--delete-emptydir-data --force，这个考试时不用加，就可以正常 draini node02 的。
>
> 但如果执行后，有跟测试环境一样的报错（如下截图），则需要加上--delete-emptydir-data --force，会强制将 pod 移除。
>
> kubectl drain node02 --ignore-daemonsets --delete-emptydir-data --force

```sh
candidate@node01:~$ kubectl drain node02 --ignore-daemonsets
node/node02 already cordoned
Warning: ignoring DaemonSet-managed Pods: calico-system/calico-node-pxcrw, kube-system/kube-proxy-ldrj7
evicting pod tigera-operator/tigera-operator-7659dcc9f4-4976s
pod/tigera-operator-7659dcc9f4-4976s evicted
node/node02 drained
```

检查

```sh
# 正常已经没有 pod 在 node02 上了。但测试环境里的 ingress、calico、kube-proxy 是 daemonsets 模式的，所以显示还在 node02 上，忽略即可。
candidate@node01:~$ kubectl get pods -A -o wide |grep node02
calico-system     calico-node-pxcrw                          1/1     Running   9 (9h ago)     110d   11.0.1.113       node02     <none>           <none>
kube-system       kube-proxy-ldrj7                           1/1     Running   0              110d   11.0.1.113       node02     <none>           <none>
tigera-operator   tigera-operator-7659dcc9f4-69gkq           1/1     Running   0              79s    11.0.1.113       node02     <none>           <none>
```

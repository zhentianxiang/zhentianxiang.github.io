---
layout: post
title: 理论-Kubernetes-12-亲和性调度
date: 2021-12-14
tags: 理论-Kubernetes
---

## 一、背景

Kubernetes中的调度策略可以大致分为两种，一种是全局的调度策略，要在启动调度器时配置，包括kubernetes调度器自带的各种predicates和priorities算法；另一种是运行时调度策略，包括nodeAffinity（主机亲和性），podAffinity（POD亲和性）以及podAntiAffinity（POD反亲和性）。

podAffinity 主要解决POD可以和哪些POD部署在同一个拓扑域中的问题（拓扑域用主机标签实现，可以是单个主机，也可以是多个主机组成的cluster、zone等），podAntiAffinity主要解决POD不能和哪些POD部署在同一个拓扑域中的问题。它们处理的是Kubernetes集群内部POD和POD之间的关系。

**与nodeSelector区别**

nodeSelector仅是针对于node节点上的标签进行调度任务，而Affinity可以通过如下内容进行调度任务：

- 更多的表达式支持，不仅仅是ADD和精确匹配了
- 可以设置soft/preference的调度策略，而不是刚性的要求
- 可以通过Pod的标签进行调度约束，不仅仅是Node的标签

## 二、使用场景

- podAntiAffinity使用场景：

将一个服务的POD分散在不同的主机或者拓扑域中，提高服务本身的稳定性。

给POD对于一个节点的独占访问权限来保证资源隔离，保证不会有其它pod来分享节点资源。

把可能会相互影响的服务的POD分散在不同的主机上。

> 对于亲和性和反亲和性，每种都有两种规则可以设置：
>
> - RequiredDuringSchedulingIgnoredDuringExecution：在调度期间要求满足亲和性或者反亲和性规则，如果不能满足规则，则POD不能被调度到对应的主机上。在之后的运行过程中，系统不会再检查这些规则是否满足。（硬规则）
>
> - PreferredDuringSchedulingIgnoredDuringExecution：在调度期间尽量满足亲和性或者反亲和性规则，如果不能满足规则，POD也有可能被调度到对应的主机上。在之后的运行过程中，系统不会再检查这些规则是否满足。（软规则）

## 三、使用示例

首先查看一下节点和节点标签

```sh
[root@k8s-kubersphere calico]# kubectl get node --show-labels
NAME              STATUS   ROLES    AGE   VERSION    LABELS
k8s-kubersphere   Ready    master   18d   v1.18.19   IngressProxy=true,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,gpuname=GTX_1080Ti,grafana=data,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-kubersphere,kubernetes.io/os=linux,node-role.kubernetes.io/master=,user=hyperdl
k8s-node01        Ready    node     18d   v1.18.19   IngressProxy=true,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node01,kubernetes.io/os=linux,node-role.kubernetes.io/node=
k8s-node02        Ready    node     18d   v1.18.19   IngressProxy=true,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node02,kubernetes.io/os=linux,node-role.kubernetes.io/node=

```

### 1. nodeAffinity（节点亲和性）

节点亲和性主要是用来控制 pod 要部署在哪些主机上，以及不能部署在哪些主机上的。它可以进行一些简单的逻辑组合了，不只是简单的相等匹配。

比如现在我们用一个 Deployment 来管理3个 pod 副本，现在我们来控制下这些 pod 的调度，如下例子

```yaml
[root@k8s-kubersphere calico]# cat nginx-ds.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    k8s-app: nginx
spec:
  type: NodePort
  selector:
    k8s-app: nginx
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 8888
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  labels:
    k8s-app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      k8s-app: nginx
  template:
    metadata:
      labels:
        k8s-app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
      restartPolicy: Always
```

默认不指定任何调度器启动，系统会自己去调式分配pod到各个节点

```sh
[root@k8s-kubersphere calico]# kubectl get pods -o wide
NAME                               READY   STATUS    RESTARTS   AGE   IP              NODE              NOMINATED NODE   READINESS GATES
nfs-provisioner-78f94f6f79-px9qs   1/1     Running   0          10d   10.100.46.62    k8s-kubersphere   <none>           <none>
nginx-86546d6646-5s9pz             1/1     Running   0          8s    10.100.58.198   k8s-node02        <none>           <none>
nginx-86546d6646-8g54t             1/1     Running   0          8s    10.100.46.60    k8s-kubersphere   <none>           <none>
nginx-86546d6646-b5f75             1/1     Running   0          8s    10.100.85.194   k8s-node01        <none>           <none>
```

但是我指定了nodeAffinity，我们就可以控制pod全部启动到同一台节点上

- requiredDuringSchedulingIgnoredDuringExecution

```yaml
[root@k8s-kubersphere calico]# cat nginx-ds.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    k8s-app: nginx
spec:
  type: NodePort
  selector:
    k8s-app: nginx
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 8888
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  labels:
    k8s-app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      k8s-app: nginx
  template:
    metadata:
      labels:
        k8s-app: nginx
    spec:
      affinity:   # 亲和性
        nodeAffinity:   #节点亲和性
          requiredDuringSchedulingIgnoredDuringExecution:  # 硬亲和
            nodeSelectorTerms:   #节点选择
            - matchExpressions:       #定义匹配表达式
              - key: kubernetes.io/hostname       #定义hostname作为拓扑域
                operator: In   #定义包含k8s-node01这个值的节点才会被调度pod
                values:
                - k8s-node01  # 定义值
      containers:
      - name: nginx
        image: nginx:latest
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
      restartPolicy: Always
```

```sh
[root@k8s-kubersphere calico]# kubectl get pods -o wide
NAME                               READY   STATUS    RESTARTS   AGE   IP              NODE              NOMINATED NODE   READINESS GATES
nfs-provisioner-78f94f6f79-px9qs   1/1     Running   0          10d   10.100.46.62    k8s-kubersphere   <none>           <none>
nginx-6c55bdb4d5-2nrjl             1/1     Running   0          23m   10.100.85.203   k8s-node01        <none>           <none>
nginx-6c55bdb4d5-7gxg2             1/1     Running   0          23m   10.100.85.209   k8s-node01        <none>           <none>
nginx-6c55bdb4d5-jcpfw             1/1     Running   0          23m   10.100.85.210   k8s-node01        <none>           <none>
```

**kubernetes操作符有以下几种**

- In：label 的值在某个列表中
- NotIn：label 的值不在某个列表中
- Gt：label 的值大于某个值
- Lt：label 的值小于某个值
- Exists：某个 label 存在
- DoesNotExist：某个 label 不存在

再次测试pod不能再k8s-node01上面，只能在k8s-node02上面

- preferredDuringSchedulingIgnoredDuringExecution

```sh
[root@k8s-kubersphere calico]# kubectl label nodes k8s-node02 app=nginx
node/k8s-node02 labeled
```

如果不配合硬规则去使用，那么pod可能会有一个俩的不会按要求去运行到指定规则的节点上，因为是软亲和规则

```yaml
spec:
  affinity:   # 亲和性
    nodeAffinity:   #节点亲和性
      preferredDuringSchedulingIgnoredDuringExecution:  # 软亲和
      - weight: 1
        preference:
          matchExpressions:
          - key: app  #定义key为app
            operator: In  #定义包nginx这个值的节点才会被调度pod
            values:
            - nginx  #定义值
```

```sh
[root@k8s-kubersphere calico]# kubectl get pods -o wide
NAME                               READY   STATUS    RESTARTS   AGE   IP              NODE              NOMINATED NODE   READINESS GATES
nfs-provisioner-78f94f6f79-px9qs   1/1     Running   0          10d   10.100.46.62    k8s-kubersphere   <none>           <none>
nginx-5c99fdc479-75j2b             1/1     Running   0          23s   10.100.58.216   k8s-node02        <none>           <none>
nginx-5c99fdc479-m6b5l             1/1     Running   0          23s   10.100.58.217   k8s-node02        <none>           <none>
nginx-5c99fdc479-tzdf9             1/1     Running   0          23s   10.100.85.220   k8s-node01        <none>           <none>
```

如果配合硬性规则去使用，那么pod会全部被调度到指定的节点上去

```yaml
spec:
  affinity:   # 亲和性
    nodeAffinity:   #节点亲和性
      requiredDuringSchedulingIgnoredDuringExecution:  # 硬策略
        nodeSelectorTerms:   #节点选择
        - matchExpressions:       #定义匹配表达式
          - key: kubernetes.io/hostname       #定义hostname作为拓扑域
            operator: NotIn   #注意，我这里是NoIn，意思是lable的值不在某个列表中，说白了就是，我这个K8s-node-01的这个值不可能不在这个列表中，也就是说我必须有这个lable的值，所以就是反着来的，所以pod没办法根据这个要求去调度，所以pod也就不会去这上面调度了，换句话说，除了这个包含k8s-node01值的节点，我pod想去哪去哪，但是下面就定义了一个软规则，就又约束了pod想去哪就去哪的心思了，下面那个就是让pod去一个key为app值含有nginx这个的节点上
            values:
            - k8s-node01 # 定义值
      preferredDuringSchedulingIgnoredDuringExecution:  # 软策略
      - weight: 1
        preference:
          matchExpressions:
          - key: app
            operator: In
            values:
            - nginx
```

```sh
[root@k8s-kubersphere calico]# kubectl get pods -o wide
NAME                               READY   STATUS    RESTARTS   AGE   IP              NODE              NOMINATED NODE   READINESS GATES
nfs-provisioner-78f94f6f79-px9qs   1/1     Running   0          10d   10.100.46.62    k8s-kubersphere   <none>           <none>
nginx-67c6546cc8-m62xg             1/1     Running   0          9s    10.100.58.209   k8s-node02        <none>           <none>
nginx-67c6546cc8-n77w4             1/1     Running   0          6s    10.100.58.222   k8s-node02        <none>           <none>
nginx-67c6546cc8-vnzbl             1/1     Running   0          11s   10.100.58.218   k8s-node02        <none>           <none>
```

**其他写法**

```yaml
spec:
  affinity:   # 亲和性
    nodeAffinity:   #节点亲和性
      requiredDuringSchedulingIgnoredDuringExecution:  # 硬策略
        nodeSelectorTerms:   #节点选择
        - matchExpressions:       #定义匹配表达式
          - key: kubernetes.io/hostname       #定义hostname作为拓扑域
            operator: NotIn   # label 的值在某个列表中，而这里的值就是k8s-node02，也就是节点，所以pod会去这上面
            values:
            - k8s-node01 # 定义值
      preferredDuringSchedulingIgnoredDuringExecution:  # 软策略
      - weight: 1
        preference:
          matchExpressions:
          - key: app
            operator: Exists  #含有这个标签，也就是含有app那个标签的节点，就可以调度pod，因为有的时候key是一致的，value需要打不同的，所以用这个操作符比较爽
            #values:
            #- nginx
```

同样pod也会被调度到一起

```sh
[root@k8s-kubersphere calico]# kubectl get pods -o wide
NAME                               READY   STATUS    RESTARTS   AGE   IP              NODE              NOMINATED NODE   READINESS GATES
nfs-provisioner-78f94f6f79-px9qs   1/1     Running   0          10d   10.100.46.62    k8s-kubersphere   <none>           <none>
nginx-86b49c7bcd-2g5hs             1/1     Running   0          4s    10.100.58.224   k8s-node02        <none>           <none>
nginx-86b49c7bcd-2n8sx             1/1     Running   0          4s    10.100.58.225   k8s-node02        <none>           <none>
nginx-86b49c7bcd-ws95f             1/1     Running   0          4s    10.100.58.223   k8s-node02        <none>           <none>
```

**小总结**

单纯使用硬亲和调度，pod会全部被归拢到同一节点，配合硬亲和调度去使用软亲和调度，pod也会被软亲和规则归拢到一起，否则单纯使用软亲和调度pod可能有一个俩的不会按着规则去调度

### 2. podffinity使用示例（Pod亲和性）

pod 亲和性主要解决 pod 可以和哪些 pod 部署在同一个拓扑域中的问题（其中拓扑域用主机标签实现，可以是单个主机，也可以是多个主机组成的 cluster、zone 等等），而 pod 反亲和性主要是解决 pod 不能和哪些 pod 部署在同一个拓扑域中的问题，它们都是处理的 pod 与 pod 之间的关系，比如一个 pod 在一个节点上了，那么我这个也得在这个节点，或者你这个 pod 在节点上了，那么我就不想和你待在同一个节点上。

由于我们这里只有一个集群，并没有区域或者机房的概念，所以我们这里直接使用主机名来作为拓扑域，把 pod 创建在同一个主机上面。


首先我这里运行了一个名为nginx-test的pod，并且运行在k8s-node02上面

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-test
  labels:
    name: nginx-pod
spec:
  replicas: 1
  selector:
    matchLabels:
      name: nginx-pod
  template:
    metadata:
      labels:
        name: nginx-pod
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
      restartPolicy: Always
```

```sh
[root@k8s-kubersphere calico]# kubectl apply -f  nginx-test.yaml
deployment.apps/nginx-test created
[root@k8s-kubersphere calico]# kubectl get pods -o wide
NAME                               READY   STATUS    RESTARTS   AGE   IP              NODE              NOMINATED NODE   READINESS GATES
nfs-provisioner-78f94f6f79-px9qs   1/1     Running   0          10d   10.100.46.62    k8s-kubersphere   <none>           <none>
nginx-test-86546d6646-48q96        1/1     Running   0          46s   10.100.58.226   k8s-node02        <none>           <none>
```

他的标签是

```sh
[root@k8s-kubersphere calico]# kubectl describe pod nginx-test-86546d6646-48q96 |grep -i Labels:
Labels:       name=nginx-pod
```

- requiredDuringSchedulingIgnoredDuringExecution:  # 硬策略

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  labels:
    k8s-app: nginx-affinity
spec:
  replicas: 3
  selector:
    matchLabels:
      k8s-app: nginx-affinity
  template:
    metadata:
      labels:
        k8s-app: nginx-affinity
    spec:
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:  # 硬亲和
          - labelSelector:
              matchExpressions:
              - key: name
                operator: In
                values:
                - nginx-pod
            topologyKey: kubernetes.io/hostname   #这里有一个注意点，因为podAffinity同nodeAffinity不一样，因为node没命名空间，而pod有，所以需要划定一个概念的范围
      containers:
      - name: nginx
        image: nginx:latest
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
      restartPolicy: Always
```
查看pod发现都追随nginx-test在同一节点上
```sh
[root@k8s-kubersphere calico]# kubectl get pods -o wide
NAME                               READY   STATUS    RESTARTS   AGE   IP              NODE              NOMINATED NODE   READINESS GATES
nfs-provisioner-78f94f6f79-px9qs   1/1     Running   0          10d   10.100.46.62    k8s-kubersphere   <none>           <none>
nginx-bdbd9645-5x749               1/1     Running   0          5s    10.100.58.221   k8s-node02        <none>           <none>
nginx-bdbd9645-d6lv5               1/1     Running   0          5s    10.100.58.229   k8s-node02        <none>           <none>
nginx-bdbd9645-wx526               1/1     Running   0          5s    10.100.58.228   k8s-node02        <none>           <none>
nginx-test-86546d6646-48q96        1/1     Running   0          10m   10.100.58.226   k8s-node02        <none>           <none>
```

接下来继续测试，测试把nginx-test删掉，然后再创建亲和性的pod，看看有什么变化

```sh
[root@k8s-kubersphere calico]# kubectl delete -f nginx-test.yaml
deployment.apps "nginx-test" deleted
[root@k8s-kubersphere calico]# kubectl get pods -o wide
NAME                               READY   STATUS    RESTARTS   AGE     IP              NODE              NOMINATED NODE   READINESS GATES
nfs-provisioner-78f94f6f79-px9qs   1/1     Running   0          10d     10.100.46.62    k8s-kubersphere   <none>           <none>
nginx-bdbd9645-5x749               1/1     Running   0          6m32s   10.100.58.221   k8s-node02        <none>           <none>
nginx-bdbd9645-d6lv5               1/1     Running   0          6m32s   10.100.58.229   k8s-node02        <none>           <none>
nginx-bdbd9645-wx526               1/1     Running   0          6m32s   10.100.58.228   k8s-node02        <none>           <none>
[root@k8s-kubersphere calico]# kubectl delete -f nginx-dp.yaml
service "nginx" deleted
deployment.apps "nginx" deleted
[root@k8s-kubersphere calico]# kubectl apply -f nginx-dp.yaml
service/nginx created
deployment.apps/nginx created
[root@k8s-kubersphere calico]# kubectl get pods -o wide
NAME                               READY   STATUS    RESTARTS   AGE   IP             NODE              NOMINATED NODE   READINESS GATES
nfs-provisioner-78f94f6f79-px9qs   1/1     Running   0          10d   10.100.46.62   k8s-kubersphere   <none>           <none>
nginx-5c4ddc4698-b8cz4             0/1     Pending   0          16s   <none>         <none>            <none>           <none>
nginx-5c4ddc4698-p54jz             0/1     Pending   0          16s   <none>         <none>            <none>           <none>
nginx-5c4ddc4698-xnms2             0/1     Pending   0          16s   <none>         <none>            <none>           <none>
```

我们可以看到处于Pending状态了，这是因为现在没有一个节点上面拥有busybox-pod这个 label 的 pod，而上面我们的调度使用的是硬策略，所以就没办法进行调度了，大家可以去尝试下重新将 test-busybox 这个 pod 调度到 node01 这个节点上，看看上面的 affinity 的3个副本会不会也被调度到 node01 这个节点上去？


好的，接下来验证上面的说法，首先修改nginx-test的调度规则，活学活用，利用nodeAffinity将pod调度到k8s-node01上面

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-test
  labels:
    name: nginx-pod
spec:
  replicas: 1
  selector:
    matchLabels:
      name: nginx-pod
  template:
    metadata:
      labels:
        name: nginx-pod
    spec:
      affinity:   # 亲和性
        nodeAffinity:   #节点亲和性
          requiredDuringSchedulingIgnoredDuringExecution:  # 硬亲和
            nodeSelectorTerms:   #节点选择
            - matchExpressions:       #定义匹配表达式
              - key: kubernetes.io/hostname       #定义hostname作为拓扑域
                operator: In   #定义包含k8s-node01这个值的节点才会被调度pod
                values:
                - k8s-node01  # 定义值
      containers:
      - name: nginx
        image: nginx:latest
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
      restartPolicy: Always
```
查看pod发现，已经调度到k8s-node01上面了，包括nginx-dp的pod也追随过去了
```sh
[root@k8s-kubersphere calico]# kubectl apply -f nginx-test.yaml
deployment.apps/nginx-test created
[root@k8s-kubersphere calico]# kubectl get pods -o wide
NAME                               READY   STATUS    RESTARTS   AGE     IP              NODE              NOMINATED NODE   READINESS GATES
nfs-provisioner-78f94f6f79-px9qs   1/1     Running   0          10d     10.100.46.62    k8s-kubersphere   <none>           <none>
nginx-cf6998557-2wgrc              1/1     Running   0          3m43s   10.100.85.212   k8s-node01        <none>           <none>
nginx-cf6998557-mtm4v              1/1     Running   0          3m43s   10.100.85.216   k8s-node01        <none>           <none>
nginx-cf6998557-z8s4p              1/1     Running   0          3m43s   10.100.85.217   k8s-node01        <none>           <none>
nginx-test-85cfd46d7b-ptpcd        1/1     Running   0          6s      10.100.85.215   k8s-node01        <none>           <none>
```

### 3. podAntiAffinity（pod反亲和性）

比如一个节点上运行了某个 pod，那么我们的 pod 则希望被调度到其他节点上去，同样我们把上面的 podAffinity 直接改成 podAntiAffinity

```yaml
spec:
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:  # 硬策略
      - labelSelector:
          matchExpressions:
          - key: name
            operator: In
            values:
            - nginx-pod
        topologyKey: kubernetes.io/hostname
```

上面的pod亲和性是将pod运行在一起，如果修改为podAntiAffinity就是，如果哪个节点上有了那种标签的pod，则，接下来生成的pod不会去那个节点


一些情况下podAntiAffinity是不让pod在同一节点上运行，而不是去躲避避开某些pod，上面的修改方式就是躲避标签key vluea为name=nginx-pod的pod去创建pod，从而不在那台机器上调度pod，如果将key vluea修改为自己要启动的pod标签，则，pod就像daemonset一样，分别在node上启动

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  labels:
    k8s-app: nginx-affinity
spec:
  replicas: 3
  selector:
    matchLabels:
      k8s-app: nginx-affinity
  template:
    metadata:
      labels:
        k8s-app: nginx-affinity
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:  # 硬策略
          - labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - nginx-affinity
            topologyKey: kubernetes.io/hostname
      containers:
      - name: nginx
        image: nginx:latest
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
      restartPolicy: Always
```

```sh
[root@k8s-kubersphere calico]# kubectl get pods -o wide
NAME                               READY   STATUS    RESTARTS   AGE     IP              NODE              NOMINATED NODE   READINESS GATES
nfs-provisioner-78f94f6f79-px9qs   1/1     Running   0          10d     10.100.46.62    k8s-kubersphere   <none>           <none>
nginx-67d687697c-d9lg9             1/1     Running   0          5m20s   10.100.46.44    k8s-kubersphere   <none>           <none>
nginx-67d687697c-lr4pd             1/1     Running   0          5m20s   10.100.58.251   k8s-node02        <none>           <none>
nginx-67d687697c-qg74h             1/1     Running   0          5m20s   10.100.85.214   k8s-node01        <none>           <none>
```

这种好处就是，对于一些hostNetwork网络模式的pod，他不会出现端口占用的情况，

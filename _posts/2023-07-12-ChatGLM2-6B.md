---
layout: post
title: 2023-07-11-ChatGLM-6B
date: 2023-07-11
tags: 其他
music-id: 447926063
---

## 一、制作Docker镜像

### 1. 克隆代码

```sh
$ git clone https://github.com/THUDM/ChatGLM-6B.git
```

### 2. 修改代码

使用CPU推理如下修改

```python
$ cd ChatGLM-6B
$ cp web_demo.py web_demo.py.bak
$ vim web_demo.py
from transformers import AutoModel, AutoTokenizer
import gradio as gr
import mdtex2html
import os

# 变量名叫MODEL_DATA,定义推理模型的数据源,默认为THUDM/chatglm-6b
model_dir = os.environ.get("MODEL_DIR", "THUDM/chatglm-6b")

# 基于CPU算法推理,并使用32G内存
bfloat_enabled = os.environ.get("BFLOAT_ENABLED", "false").lower() == "true"
# 基于CPU算法推理,并使用16G内存
bfloat16_enabled = os.environ.get("BFLOAT16_ENABLED", "true").lower() == "true"


tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
model = AutoModel.from_pretrained(model_dir, trust_remote_code=True)

# 判断 BFLOAT16_ENABLED 变量，如果为 "true"，则开启 bfloat16；否则，继续判断 BFLOAT_ENABLED 变量，如果为 "true"，则开启 bfloat。如果两个变量都不为 "true"，则不做任何操作
if bfloat16_enabled:
    model = model.bfloat16()
elif bfloat_enabled:
    model = model.bfloat()

model = model.eval()

"""Override Chatbot.postprocess"""


def postprocess(self, y):
    if y is None:
        return []
    for i, (message, response) in enumerate(y):
        y[i] = (
            None if message is None else mdtex2html.convert((message)),
            None if response is None else mdtex2html.convert(response),
        )
    return y


gr.Chatbot.postprocess = postprocess


def parse_text(text):
    """copy from https://github.com/GaiZhenbiao/ChuanhuChatGPT/"""
    lines = text.split("\n")
    lines = [line for line in lines if line != ""]
    count = 0
    for i, line in enumerate(lines):
        if "```" in line:
            count += 1
            items = line.split('`')
            if count % 2 == 1:
                lines[i] = f'<pre><code class="language-{items[-1]}">'
            else:
                lines[i] = f'<br></code></pre>'
        else:
            if i > 0:
                if count % 2 == 1:
                    line = line.replace("`", "\`")
                    line = line.replace("<", "&lt;")
                    line = line.replace(">", "&gt;")
                    line = line.replace(" ", "&nbsp;")
                    line = line.replace("*", "&ast;")
                    line = line.replace("_", "&lowbar;")
                    line = line.replace("-", "&#45;")
                    line = line.replace(".", "&#46;")
                    line = line.replace("!", "&#33;")
                    line = line.replace("(", "&#40;")
                    line = line.replace(")", "&#41;")
                    line = line.replace("$", "&#36;")
                lines[i] = "<br>"+line
    text = "".join(lines)
    return text


def predict(input, chatbot, max_length, top_p, temperature, history):
    chatbot.append((parse_text(input), ""))
    for response, history in model.stream_chat(tokenizer, input, history, max_length=max_length, top_p=top_p,
                                               temperature=temperature):
        chatbot[-1] = (parse_text(input), parse_text(response))       

        yield chatbot, history


def reset_user_input():
    return gr.update(value='')


def reset_state():
    return [], []


with gr.Blocks() as demo:
    gr.HTML("""<h1 align="center">ChatGLM</h1>""")

    chatbot = gr.Chatbot()
    with gr.Row():
        with gr.Column(scale=4):
            with gr.Column(scale=12):
                user_input = gr.Textbox(show_label=False, placeholder="Input...", lines=10).style(
                    container=False)
            with gr.Column(min_width=32, scale=1):
                submitBtn = gr.Button("Submit", variant="primary")
        with gr.Column(scale=1):
            emptyBtn = gr.Button("Clear History")
            max_length = gr.Slider(0, 4096, value=2048, step=1.0, label="Maximum length", interactive=True)
            top_p = gr.Slider(0, 1, value=0.7, step=0.01, label="Top P", interactive=True)
            temperature = gr.Slider(0, 1, value=0.95, step=0.01, label="Temperature", interactive=True)

    history = gr.State([])

    submitBtn.click(predict, [user_input, chatbot, max_length, top_p, temperature, history], [chatbot, history],
                    show_progress=True)
    submitBtn.click(reset_user_input, [], [user_input])

    emptyBtn.click(reset_state, outputs=[chatbot, history], show_progress=True)

#demo.queue().launch(share=True, inbrowser=True, server_name="0.0.0.0" ,server_port=7860)
os.environ["no_proxy"] = "localhost,127.0.0.1,::1"
demo.queue().launch(server_name="0.0.0.0", server_port=8888)
```

使用GPU推理如下修改

```python
$ cd ChatGLM-6B
$ cp web_demo.py web_demo.py.bak
$ vim web_demo.py
from transformers import AutoModel, AutoTokenizer
import gradio as gr
import mdtex2html
import os

# 变量名叫MODEL_DATA,定义推理模型的数据源,默认为THUDM/chatglm-6b
model_dir = os.environ.get("MODEL_DIR", "THUDM/chatglm-6b")

# 基于GPU算法推理,使用量化int4精度加载，需要6G显存
quantize4_enabled = os.environ.get("QUANTIZE4_ENABLED", "false").lower() == "true"
# 基于GPU算法推理,使用量化int8精度加载，需要10G显存
quantize8_enabled = os.environ.get("QUANTIZE8_ENABLED", "false").lower() == "true"

tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
model = AutoModel.from_pretrained(model_dir, trust_remote_code=True)

# 将根据环境变量 QUANTIZE4_ENABLED 和 QUANTIZE8_ENABLED 的值来选择是否执行量化操作。如果 QUANTIZE4_ENABLED 为 "true"，则对模型进行 4 位量化；如果 QUANTIZE8_ENABLED 为 "true"，则对模型进行 8 位量化。如果两个变量都不为 "true"，则将模型设为半精度 (half()) 并移动到 CUDA 上进行加速 (cuda())
if quantize4_enabled:
    model = model.quantize(4).cuda()
elif quantize8_enabled:
    model = model.quantize(8).cuda()
else:
    model = model.half().cuda()

model = model.eval()

"""Override Chatbot.postprocess"""


def postprocess(self, y):
    if y is None:
        return []
    for i, (message, response) in enumerate(y):
        y[i] = (
            None if message is None else mdtex2html.convert((message)),
            None if response is None else mdtex2html.convert(response),
        )
    return y


gr.Chatbot.postprocess = postprocess


def parse_text(text):
    """copy from https://github.com/GaiZhenbiao/ChuanhuChatGPT/"""
    lines = text.split("\n")
    lines = [line for line in lines if line != ""]
    count = 0
    for i, line in enumerate(lines):
        if "```" in line:
            count += 1
            items = line.split('`')
            if count % 2 == 1:
                lines[i] = f'<pre><code class="language-{items[-1]}">'
            else:
                lines[i] = f'<br></code></pre>'
        else:
            if i > 0:
                if count % 2 == 1:
                    line = line.replace("`", "\`")
                    line = line.replace("<", "&lt;")
                    line = line.replace(">", "&gt;")
                    line = line.replace(" ", "&nbsp;")
                    line = line.replace("*", "&ast;")
                    line = line.replace("_", "&lowbar;")
                    line = line.replace("-", "&#45;")
                    line = line.replace(".", "&#46;")
                    line = line.replace("!", "&#33;")
                    line = line.replace("(", "&#40;")
                    line = line.replace(")", "&#41;")
                    line = line.replace("$", "&#36;")
                lines[i] = "<br>"+line
    text = "".join(lines)
    return text


def predict(input, chatbot, max_length, top_p, temperature, history):
    chatbot.append((parse_text(input), ""))
    for response, history in model.stream_chat(tokenizer, input, history, max_length=max_length, top_p=top_p,
                                               temperature=temperature):
        chatbot[-1] = (parse_text(input), parse_text(response))       

        yield chatbot, history


def reset_user_input():
    return gr.update(value='')


def reset_state():
    return [], []


with gr.Blocks() as demo:
    gr.HTML("""<h1 align="center">ChatGLM</h1>""")

    chatbot = gr.Chatbot()
    with gr.Row():
        with gr.Column(scale=4):
            with gr.Column(scale=12):
                user_input = gr.Textbox(show_label=False, placeholder="Input...", lines=10).style(
                    container=False)
            with gr.Column(min_width=32, scale=1):
                submitBtn = gr.Button("Submit", variant="primary")
        with gr.Column(scale=1):
            emptyBtn = gr.Button("Clear History")
            max_length = gr.Slider(0, 4096, value=2048, step=1.0, label="Maximum length", interactive=True)
            top_p = gr.Slider(0, 1, value=0.7, step=0.01, label="Top P", interactive=True)
            temperature = gr.Slider(0, 1, value=0.95, step=0.01, label="Temperature", interactive=True)

    history = gr.State([])

    submitBtn.click(predict, [user_input, chatbot, max_length, top_p, temperature, history], [chatbot, history],
                    show_progress=True)
    submitBtn.click(reset_user_input, [], [user_input])

    emptyBtn.click(reset_state, outputs=[chatbot, history], show_progress=True)

#demo.queue().launch(share=True, inbrowser=True, server_name="0.0.0.0" ,server_port=7860)
os.environ["no_proxy"] = "localhost,127.0.0.1,::1"
demo.queue().launch(server_name="0.0.0.0", server_port=8888)
```

### 3. build

CPU的如下

```dockerfile
FROM python:3.10

COPY ChatGLM-6B /ChatGLM-6B-CPU

WORKDIR /ChatGLM-6B-CPU

RUN pip install -r requirements.txt

CMD ["python", "web_demo.py"]
```

```sh
$ docker build . -t chatglm-6b-cpu:v1.0.0
```

GPU的如下

```dockerfile
FROM nvidia/cuda:12.2.0-devel-ubuntu20.04

COPY ChatGLM-6B /ChatGLM-6B-GPU

WORKDIR /ChatGLM-6B-GPU

RUN apt-get update && apt-get install -y python3 python3-pip

RUN ln -sv /usr/bin/python3 /usr/bin/python && \
    python -m pip install --upgrade pip && \
    pip install -r requirements.txt

CMD ["python", "web_demo.py"]
```

```sh
$ docker build . -t chatglm-6b-gpu:v1.0.0
```

## 二、下载模型数据

下载地址：https://huggingface.co/THUDM/chatglm-6b/tree/main

```sh
# 使用脚本来下载模型文件
$ vim parallel_download.py
import os
import requests
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm

# 定义下载链接的基本路径
BASE_URL = "https://huggingface.co/THUDM/chatglm-6b/resolve/main/"

# 定义要下载的文件列表
FILES = [
    ".gitattributes",
    "LICENSE",
    "MODEL_LICENSE",
    "README.md",
    "config.json",
    "configuration_chatglm.py",
    "ice_text.model",
    "modeling_chatglm.py",
    "pytorch_model-00001-of-00008.bin",
    "pytorch_model-00002-of-00008.bin",
    "pytorch_model-00003-of-00008.bin",
    "pytorch_model-00004-of-00008.bin",
    "pytorch_model-00005-of-00008.bin",
    "pytorch_model-00006-of-00008.bin",
    "pytorch_model-00007-of-00008.bin",
    "pytorch_model-00008-of-00008.bin",
    "pytorch_model.bin.index.json",
    "quantization.py",
    "test_modeling_chatglm.py",
    "tokenization_chatglm.py",
    "tokenizer_config.json"
]

# 定义并创建保存文件的目录
OUTPUT_DIR = "./model"
os.makedirs(OUTPUT_DIR, exist_ok=True)

def download_file(file):
    url = BASE_URL + file
    output_file = os.path.join(OUTPUT_DIR, file)

    # 获取已下载的文件大小
    file_size = os.path.getsize(output_file) if os.path.exists(output_file) else 0
    
    # 发送 HTTP 请求并保存文件
    headers = {"Range": f"bytes={file_size}-"}
    response = requests.get(url, headers=headers, stream=True)
    response.raise_for_status()

    # 使用 tqdm 进度条显示下载进度
    total_size = int(response.headers.get("content-length", 0)) + file_size
    progress_bar = tqdm(total=total_size, initial=file_size, unit="B", unit_scale=True)
    
    with open(output_file, "ab") as f:
        for chunk in response.iter_content(chunk_size=8192):
            if chunk:
                f.write(chunk)
                progress_bar.update(len(chunk))
    
    progress_bar.close()

# 使用线程池进行并行下载
with ThreadPoolExecutor() as executor:
    results = [executor.submit(download_file, file) for file in FILES]
    
    # 等待所有任务完成
    for result in results:
        result.result()

print("下载完成！")

$ python3 parallel_download.py


$ ls -lh model
total 12G
-rw-r--r-- 1 root root  1.2K Jul 19 14:18 config.json
-rw-r--r-- 1 root root  2.2K Jul 19 14:18 configuration_chatglm.py
-rw-r--r-- 1 root root   50K Jul 19 14:18 modeling_chatglm.py
-rw-r--r-- 1 root root  4.1K Jul 19 14:18 MODEL_LICENSE
-rw-r--r-- 1 root root  1.8G Jun 25 01:01 pytorch_model-00001-of-00007.bin
-rw-r--r-- 1 root root  1.9G Jun 25 01:10 pytorch_model-00002-of-00007.bin
-rw-r--r-- 1 root root  1.8G Jun 25 00:27 pytorch_model-00003-of-00007.bin
-rw-r--r-- 1 root root  1.7G Jun 25 01:04 pytorch_model-00004-of-00007.bin
-rw-r--r-- 1 root root  1.9G Jun 25 01:13 pytorch_model-00005-of-00007.bin
-rw-r--r-- 1 root root  1.8G Jun 25 01:07 pytorch_model-00006-of-00007.bin
-rw-r--r-- 1 root root 1005M Jun 25 00:31 pytorch_model-00007-of-00007.bin
-rw-r--r-- 1 root root   20K Jul 19 16:11 pytorch_model.bin.index.json
-rw-r--r-- 1 root root   15K Jul 19 16:11 quantization.py
-rw-r--r-- 1 root root  8.0K Jul 19 14:18 README.md
-rw-r--r-- 1 root root  9.9K Jul 19 16:11 tokenization_chatglm.py
-rw-r--r-- 1 root root   244 Jul 19 16:11 tokenizer_config.json
-rw-r--r-- 1 root root  995K Jun 25 13:33 tokenizer.model
```

## 三、启动容器

### 1. CPU推理启动

> MODEL_DIR  模型文件存储位置
>
> BFLOAT16_ENABLED=true  开启16G内存进行推理
>
> BFLOAT_ENABLED=true 开启32G内存进行推理

```sh
$ docker run -dit --name chatglm-6b-cpu -p 7860:7860 -v $(pwd)/model:/model -e MODEL_DIR=/model -e BFLOAT16_ENABLED=true chatglm-6b-cpu:v1.0.0
```

### 2. GPU推理启动

> QUANTIZE4_ENABLED="true"  对模型进行 4 位量化，大约消耗6G显存
>
> QUANTIZE8_ENABLED="true"  对模型进行 8 位量化，大约消耗10G显存
>
> 两个变量只能二选一，如果两个都不写则将模型设为半精度 (half()) 并移动到 CUDA 上进行加速 (cuda())

```sh
$ docker run -dit --name chatglm-6b-gpu -p 7860:7860 -v $(pwd)/model:/model -e MODEL_DIR=/model -e QUANTIZE4_ENABLED=true chatglm-6b-gpu:v1.0.0
```

### 3. 查看服务

```sh
$ docker ps -l
CONTAINER ID   IMAGE                COMMAND                CREATED         STATUS         PORTS     NAMES
5c3c42bc28eb   chatglm-6b-cpu:v1.0.0   "python web_demo.py"   4 seconds ago   Up 3 seconds             chatglm-6b-cpu
$ docker logs chatglm-6b-cpu
Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.
Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.
Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:17<00:00,  2.56s/it]
/usr/local/lib/python3.10/site-packages/gradio/components/textbox.py:259: UserWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.
  warnings.warn(
Running on local URL:  http://0.0.0.0:7860

Could not create share link. Please check your internet connection or our status page: https://status.gradio.app. 

Also please ensure that your antivirus or firewall is not blocking the binary file located at: /usr/local/lib/python3.10/site-packages/gradio/frpc_linux_amd64_v0.2
```
### 4. 录屏演示

该录屏中模型回答速度如此之快使用为用的RTX3090显卡进行的推理,现存消耗为13G

<video width="1200" height="600" controls>
    <source src="https://blog.linuxtian.top/data/1-jekyll%E5%8D%9A%E5%AE%A2%E7%9B%B8%E5%85%B3/2023-07-11-ChatGLM2-6B/ChatGLM2-6B.mp4">
</video>
